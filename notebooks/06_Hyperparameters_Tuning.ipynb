{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve selected our classification models, but we can't dive right into classification. The next challenge is to optimize the model construction. Since we’re working with a small dataset, the main risk is overfitting. To address this, we’ll apply hyperparameter tuning using **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from CogniPredictAD.visualization import Visualizer\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 116)\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.set_option(\"display.max_info_columns\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "Open the training dataset with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>mPACCdigit</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>FDG</th>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>108</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>63.63640</td>\n",
       "      <td>-4.84005</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>2.428570</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.833330</td>\n",
       "      <td>2.75000</td>\n",
       "      <td>1.222830</td>\n",
       "      <td>0.040838</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.706210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.42702</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.161970</td>\n",
       "      <td>0.020445</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>0.752850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>300</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>-18.90540</td>\n",
       "      <td>2.300</td>\n",
       "      <td>1.844446</td>\n",
       "      <td>1.248572</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.366668</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.841666</td>\n",
       "      <td>2.847620</td>\n",
       "      <td>3.033334</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.166668</td>\n",
       "      <td>3.80000</td>\n",
       "      <td>0.924559</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>0.522572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1.5</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>63</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>85.71430</td>\n",
       "      <td>-7.95749</td>\n",
       "      <td>1.925</td>\n",
       "      <td>1.269446</td>\n",
       "      <td>1.166668</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.466668</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.891666</td>\n",
       "      <td>1.272222</td>\n",
       "      <td>1.066668</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.733332</td>\n",
       "      <td>2.10000</td>\n",
       "      <td>1.119130</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.013942</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.637729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>7.14286</td>\n",
       "      <td>-1.94841</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.833330</td>\n",
       "      <td>1.25000</td>\n",
       "      <td>1.279034</td>\n",
       "      <td>0.026879</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>0.635279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>18.18180</td>\n",
       "      <td>2.22837</td>\n",
       "      <td>1.500</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>1.416100</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.710296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>-9.28099</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>2.428570</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.268520</td>\n",
       "      <td>0.080942</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>0.711762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>42.85710</td>\n",
       "      <td>-2.30539</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.456170</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>0.711396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>-1.42719</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.222220</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.66667</td>\n",
       "      <td>1.318880</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.026801</td>\n",
       "      <td>0.663416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.92137</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.444440</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.251688</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.705238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DX  AGE  PTGENDER  PTEDUCAT  APOE4  MMSE  CDRSB  ADAS13  LDELTOTAL  FAQ  \\\n",
       "0      2   77         0        16      1    28    2.5       5          1    0   \n",
       "1      0   59         1        16      1    30    0.0       0         19    0   \n",
       "2      3   77         1        12      2    22    8.0      30          0   25   \n",
       "3      2   82         1        20      0    26    1.5      21          4    0   \n",
       "4      0   83         0        17      0    27    0.0       5         13    3   \n",
       "...   ..  ...       ...       ...    ...   ...    ...     ...        ...  ...   \n",
       "1929   0   72         0        18      1    30    0.0       4         11    0   \n",
       "1930   3   72         0        12      1    26    7.0      29          5   18   \n",
       "1931   0   70         0        17      0    29    0.0      23         10    0   \n",
       "1932   0   84         1        12      0    30    0.5      16         13    0   \n",
       "1933   0   79         0        20      0    29    0.0       6         15    0   \n",
       "\n",
       "      MOCA  TRABSCOR  RAVLT_immediate  RAVLT_learning  RAVLT_perc_forgetting  \\\n",
       "0       24       108               47               5               63.63640   \n",
       "1       30        47               71               2                0.00000   \n",
       "2       17       300               19               1              100.00000   \n",
       "3       24        63               35               1               85.71430   \n",
       "4       25        98               57               7                7.14286   \n",
       "...    ...       ...              ...             ...                    ...   \n",
       "1929    26        52               42               7               18.18180   \n",
       "1930    19        67               34              -1              100.00000   \n",
       "1931    20       300               31               4               42.85710   \n",
       "1932    26        65               27               1               80.00000   \n",
       "1933    28        77               50               3                0.00000   \n",
       "\n",
       "      mPACCdigit  EcogPtMem  EcogPtLang  EcogPtVisspat  EcogPtPlan  \\\n",
       "0       -4.84005      2.250    2.111110       1.000000        1.00   \n",
       "1        5.42702      1.000    1.000000       1.000000        1.00   \n",
       "2      -18.90540      2.300    1.844446       1.248572        1.58   \n",
       "3       -7.95749      1.925    1.269446       1.166668        1.20   \n",
       "4       -1.94841      1.250    1.333330       1.000000        1.00   \n",
       "...          ...        ...         ...            ...         ...   \n",
       "1929     2.22837      1.500    2.333330       1.285710        1.00   \n",
       "1930    -9.28099      1.500    1.000000       1.142860        1.00   \n",
       "1931    -2.30539      1.125    1.111110       1.000000        1.00   \n",
       "1932    -1.42719      2.000    2.000000       2.000000        2.00   \n",
       "1933     1.92137      1.625    1.444440       1.500000        1.00   \n",
       "\n",
       "      EcogPtOrgan  EcogPtDivatt  EcogSPMem  EcogSPLang  EcogSPVisspat  \\\n",
       "0        1.333330          1.00   2.375000    2.111110       2.428570   \n",
       "1        1.000000          1.00   1.000000    1.000000       1.000000   \n",
       "2        1.366668          1.75   3.841666    2.847620       3.033334   \n",
       "3        1.466668          1.60   1.891666    1.272222       1.066668   \n",
       "4        1.333330          1.00   1.375000    1.111110       1.666670   \n",
       "...           ...           ...        ...         ...            ...   \n",
       "1929     2.500000          1.25   1.250000    1.000000       1.200000   \n",
       "1930     1.000000          1.00   3.250000    2.333330       2.428570   \n",
       "1931     1.000000          1.00   1.000000    1.000000       1.000000   \n",
       "1932     1.500000          2.00   1.625000    1.222220       1.285710   \n",
       "1933     1.000000          1.25   1.125000    1.000000       1.000000   \n",
       "\n",
       "      EcogSPPlan  EcogSPOrgan  EcogSPDivatt       FDG  PTAU/ABETA  \\\n",
       "0           2.60     2.833330       2.75000  1.222830    0.040838   \n",
       "1           1.00     1.000000       1.00000  1.161970    0.020445   \n",
       "2           2.97     3.166668       3.80000  0.924559    0.047131   \n",
       "3           1.16     1.733332       2.10000  1.119130    0.020198   \n",
       "4           1.00     1.833330       1.25000  1.279034    0.026879   \n",
       "...          ...          ...           ...       ...         ...   \n",
       "1929        1.00     1.333330       1.50000  1.416100    0.013555   \n",
       "1930        3.20     3.000000       3.50000  1.268520    0.080942   \n",
       "1931        1.00     1.000000       1.00000  1.456170    0.007661   \n",
       "1932        1.25     1.000000       1.66667  1.318880    0.021033   \n",
       "1933        1.00     1.000000       1.00000  1.251688    0.022828   \n",
       "\n",
       "      Hippocampus/ICV  Entorhinal/ICV  Fusiform/ICV  MidTemp/ICV  \\\n",
       "0            0.004524        0.001882      0.012107     0.011311   \n",
       "1            0.004452        0.002756      0.012935     0.014299   \n",
       "2            0.002825        0.001348      0.010049     0.009701   \n",
       "3            0.003736        0.002083      0.013038     0.013942   \n",
       "4            0.004611        0.002170      0.011387     0.012975   \n",
       "...               ...             ...           ...          ...   \n",
       "1929         0.005079        0.003304      0.014043     0.013729   \n",
       "1930         0.004383        0.001691      0.011582     0.011346   \n",
       "1931         0.005042        0.002406      0.013522     0.013008   \n",
       "1932         0.004567        0.002176      0.012360     0.013614   \n",
       "1933         0.004983        0.002936      0.012679     0.014308   \n",
       "\n",
       "      Ventricles/ICV  WholeBrain/ICV  \n",
       "0           0.016977        0.706210  \n",
       "1           0.025614        0.752850  \n",
       "2           0.053417        0.522572  \n",
       "3           0.024176        0.637729  \n",
       "4           0.052196        0.635279  \n",
       "...              ...             ...  \n",
       "1929        0.027992        0.710296  \n",
       "1930        0.022499        0.711762  \n",
       "1931        0.013065        0.711396  \n",
       "1932        0.026801        0.663416  \n",
       "1933        0.020760        0.705238  \n",
       "\n",
       "[1934 rows x 36 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open the dataset with pandas\n",
    "dataset = pd.read_csv(\"../data/train.csv\")\n",
    "viz = Visualizer(dataset)\n",
    "dataset.shape\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion about CDRSB, LDELTOTAL, and mPACCdigit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `CDRSB`, `LDELTOTAL`, and `mPACCdigit` have the highest scores in the **SelectKBest** method with **Kruskal–Wallis H-test**, as highlighted in the *Preprocessing Notebook*.\n",
    "- `CDRSB`: **H-statistic = 1612.9154; p-value < 1 e-308**\n",
    "- `LDELTOTAL`: **H-statistic = 1479.4388; p-value < 1 e-308**\n",
    "- `mPACCdigit`: **H-statistic = 1366.8357; p-value = 4.629 e-296**\n",
    "\n",
    "This raises the potential phenomenon of **sparse features**, where a few variables dominate the model, while many others contribute negligibly to the prediction. This can lead to a risk of **local overfitting**, with models that perform very well on the training dataset (here ADNIMERGE), but whose accuracy may decrease on external data.\n",
    "\n",
    "These observations are also supported by the literature: [the study Kauppi et al., 2020 identifies `CDRSB`, `LDELTOTAL`, and `mPACCdigit` among the most important features for predicting disease diagnosis](https://www.medrxiv.org/content/10.1101/2020.11.09.20226746v3.full). This features may be highly predictive in selected cohorts such as ADNI, but their performance could tend to decline in more heterogeneous clinical populations, or with other datasets. \n",
    "\n",
    "In summary, this is not an intrinsic flaw in cognitive tests, but rather a possible **dataset bias**: the observed strong accuracy could reflect the specific structure of ADNI rather than universal predictive validity. \n",
    "\n",
    "**Since we can't determine this, I believe the best course of action is to create a predictive model that includes `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, and a model that ignores them. If these three features prove more efficient at predicting only within this sample, we would still have a predictive model that tends to ignore them and is therefore still useful for prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset with Hybrid Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution (count):\n",
      "DX\n",
      "0    717\n",
      "2    548\n",
      "1    336\n",
      "3    333\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original class distribution (percentages):\n",
      "DX\n",
      "0    37.07\n",
      "2    28.34\n",
      "1    17.37\n",
      "3    17.22\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original class distribution (count):\")\n",
    "print(dataset['DX'].value_counts())\n",
    "print(\"\\nOriginal class distribution (percentages):\")\n",
    "print((dataset['DX'].value_counts(normalize=True) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oversample dict (SMOTENC) -> classes to increase:\n",
      "{1: 500, 3: 500}\n",
      "\n",
      "Undersample dict (RUS) -> classes to reduce:\n",
      "{0: 500, 2: 500}\n"
     ]
    }
   ],
   "source": [
    "# Oversampling strategy: only for classes smaller than target_count\n",
    "oversample_dict = {1: 500, 3: 500}\n",
    "\n",
    "# Undersampling strategy: only for classes larger than target_count\n",
    "undersample_dict = {0: 500, 2: 500}\n",
    "\n",
    "categorical_features = [\n",
    "    dataset.columns.get_loc(\"PTGENDER\"),\n",
    "    dataset.columns.get_loc(\"APOE4\")\n",
    "]\n",
    "\n",
    "print(\"\\nOversample dict (SMOTENC) -> classes to increase:\")\n",
    "print(oversample_dict)\n",
    "print(\"\\nUndersample dict (RUS) -> classes to reduce:\")\n",
    "print(undersample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution after resampling (count):\n",
      "DX\n",
      "0    500\n",
      "1    500\n",
      "2    500\n",
      "3    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after resampling (percentages):\n",
      "DX\n",
      "0    25.0\n",
      "1    25.0\n",
      "2    25.0\n",
      "3    25.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>72.625000</td>\n",
       "      <td>7.289646</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTGENDER</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.500119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>15.980000</td>\n",
       "      <td>2.755793</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOE4</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.555500</td>\n",
       "      <td>0.645855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMSE</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>26.977000</td>\n",
       "      <td>2.837163</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDRSB</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.830313</td>\n",
       "      <td>1.862065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.490204</td>\n",
       "      <td>2.997755</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAS13</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>17.348000</td>\n",
       "      <td>9.914335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>6.849500</td>\n",
       "      <td>5.231686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAQ</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.714500</td>\n",
       "      <td>6.493233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOCA</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>22.194000</td>\n",
       "      <td>4.341173</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRABSCOR</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>127.518000</td>\n",
       "      <td>79.514388</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>162.250000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>34.773000</td>\n",
       "      <td>12.546162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>43.250000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.112500</td>\n",
       "      <td>2.744560</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>60.582138</td>\n",
       "      <td>34.473956</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>30.769200</td>\n",
       "      <td>61.237736</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mPACCdigit</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>-6.574144</td>\n",
       "      <td>6.449618</td>\n",
       "      <td>-23.603400</td>\n",
       "      <td>-11.514300</td>\n",
       "      <td>-5.462946</td>\n",
       "      <td>-1.438387</td>\n",
       "      <td>6.300310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtMem</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.167462</td>\n",
       "      <td>0.631984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.556251</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtLang</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.792493</td>\n",
       "      <td>0.575054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.683389</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.444750</td>\n",
       "      <td>0.490849</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.092084</td>\n",
       "      <td>1.314286</td>\n",
       "      <td>1.634818</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.456198</td>\n",
       "      <td>0.494954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.667234</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.544248</td>\n",
       "      <td>0.535541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.447997</td>\n",
       "      <td>1.766668</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.845133</td>\n",
       "      <td>0.654182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.351124</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.170100</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPMem</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.246925</td>\n",
       "      <td>0.888777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.464286</td>\n",
       "      <td>2.151786</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPLang</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.729492</td>\n",
       "      <td>0.701888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.222220</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.596980</td>\n",
       "      <td>0.713291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.976416</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.706037</td>\n",
       "      <td>0.772525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.815952</td>\n",
       "      <td>0.838416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.033334</td>\n",
       "      <td>1.544486</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.022717</td>\n",
       "      <td>0.880413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>2.718662</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FDG</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.193025</td>\n",
       "      <td>0.150750</td>\n",
       "      <td>0.647497</td>\n",
       "      <td>1.096885</td>\n",
       "      <td>1.206304</td>\n",
       "      <td>1.298354</td>\n",
       "      <td>1.735930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>0.027922</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.030044</td>\n",
       "      <td>0.054819</td>\n",
       "      <td>0.245297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.005104</td>\n",
       "      <td>0.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.005052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.011476</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.005934</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>0.011465</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.017211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.012851</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.011542</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>0.014076</td>\n",
       "      <td>0.019315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.026419</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>0.078630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.676922</td>\n",
       "      <td>0.052579</td>\n",
       "      <td>0.507694</td>\n",
       "      <td>0.638098</td>\n",
       "      <td>0.679397</td>\n",
       "      <td>0.716086</td>\n",
       "      <td>0.819394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DX</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.118314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count        mean        std        min        25%  \\\n",
       "AGE                    2000.0   72.625000   7.289646  52.000000  68.000000   \n",
       "PTGENDER               2000.0    0.497500   0.500119   0.000000   0.000000   \n",
       "PTEDUCAT               2000.0   15.980000   2.755793   4.000000  14.000000   \n",
       "APOE4                  2000.0    0.555500   0.645855   0.000000   0.000000   \n",
       "MMSE                   2000.0   26.977000   2.837163  16.000000  25.000000   \n",
       "CDRSB                  2000.0    1.830313   1.862065   0.000000   0.500000   \n",
       "ADAS13                 2000.0   17.348000   9.914335   0.000000  10.000000   \n",
       "LDELTOTAL              2000.0    6.849500   5.231686   0.000000   2.000000   \n",
       "FAQ                    2000.0    4.714500   6.493233   0.000000   0.000000   \n",
       "MOCA                   2000.0   22.194000   4.341173   4.000000  20.000000   \n",
       "TRABSCOR               2000.0  127.518000  79.514388  21.000000  69.000000   \n",
       "RAVLT_immediate        2000.0   34.773000  12.546162   0.000000  25.000000   \n",
       "RAVLT_learning         2000.0    4.112500   2.744560  -4.000000   2.000000   \n",
       "RAVLT_perc_forgetting  2000.0   60.582138  34.473956 -50.000000  30.769200   \n",
       "mPACCdigit             2000.0   -6.574144   6.449618 -23.603400 -11.514300   \n",
       "EcogPtMem              2000.0    2.167462   0.631984   1.000000   1.725000   \n",
       "EcogPtLang             2000.0    1.792493   0.575054   1.000000   1.333330   \n",
       "EcogPtVisspat          2000.0    1.444750   0.490849   1.000000   1.092084   \n",
       "EcogPtPlan             2000.0    1.456198   0.494954   1.000000   1.000000   \n",
       "EcogPtOrgan            2000.0    1.544248   0.535541   1.000000   1.166670   \n",
       "EcogPtDivatt           2000.0    1.845133   0.654182   1.000000   1.351124   \n",
       "EcogSPMem              2000.0    2.246925   0.888777   1.000000   1.464286   \n",
       "EcogSPLang             2000.0    1.729492   0.701888   1.000000   1.111110   \n",
       "EcogSPVisspat          2000.0    1.596980   0.713291   1.000000   1.000000   \n",
       "EcogSPPlan             2000.0    1.706037   0.772525   1.000000   1.000000   \n",
       "EcogSPOrgan            2000.0    1.815952   0.838416   1.000000   1.033334   \n",
       "EcogSPDivatt           2000.0    2.022717   0.880413   1.000000   1.250000   \n",
       "FDG                    2000.0    1.193025   0.150750   0.647497   1.096885   \n",
       "PTAU/ABETA             2000.0    0.038284   0.027922   0.006020   0.016570   \n",
       "Hippocampus/ICV        2000.0    0.004501   0.000825   0.001998   0.003891   \n",
       "Entorhinal/ICV         2000.0    0.002351   0.000553   0.000926   0.001959   \n",
       "Fusiform/ICV           2000.0    0.011476   0.001714   0.005934   0.010275   \n",
       "MidTemp/ICV            2000.0    0.012851   0.001857   0.006737   0.011542   \n",
       "Ventricles/ICV         2000.0    0.026419   0.012514   0.004600   0.017095   \n",
       "WholeBrain/ICV         2000.0    0.676922   0.052579   0.507694   0.638098   \n",
       "DX                     2000.0    1.500000   1.118314   0.000000   0.750000   \n",
       "\n",
       "                             50%         75%         max  \n",
       "AGE                    73.000000   78.000000   90.000000  \n",
       "PTGENDER                0.000000    1.000000    1.000000  \n",
       "PTEDUCAT               16.000000   18.000000   20.000000  \n",
       "APOE4                   0.000000    1.000000    2.000000  \n",
       "MMSE                   28.000000   29.000000   30.000000  \n",
       "CDRSB                   1.490204    2.997755   10.000000  \n",
       "ADAS13                 15.000000   24.000000   56.000000  \n",
       "LDELTOTAL               7.000000   10.000000   22.000000  \n",
       "FAQ                     1.000000    8.000000   30.000000  \n",
       "MOCA                   23.000000   25.000000   30.000000  \n",
       "TRABSCOR               97.500000  162.250000  300.000000  \n",
       "RAVLT_immediate        34.000000   43.250000   71.000000  \n",
       "RAVLT_learning          4.000000    6.000000   12.000000  \n",
       "RAVLT_perc_forgetting  61.237736  100.000000  100.000000  \n",
       "mPACCdigit             -5.462946   -1.438387    6.300310  \n",
       "EcogPtMem               2.125000    2.556251    4.000000  \n",
       "EcogPtLang              1.683389    2.111110    4.000000  \n",
       "EcogPtVisspat           1.314286    1.634818    4.000000  \n",
       "EcogPtPlan              1.360000    1.667234    4.000000  \n",
       "EcogPtOrgan             1.447997    1.766668    4.000000  \n",
       "EcogPtDivatt            1.750000    2.170100    4.000000  \n",
       "EcogSPMem               2.151786    3.000000    4.000000  \n",
       "EcogSPLang              1.500000    2.222220    4.000000  \n",
       "EcogSPVisspat           1.285710    1.976416    4.000000  \n",
       "EcogSPPlan              1.410000    2.160000    4.000000  \n",
       "EcogSPOrgan             1.544486    2.333330    4.000000  \n",
       "EcogSPDivatt            1.850000    2.718662    4.000000  \n",
       "FDG                     1.206304    1.298354    1.735930  \n",
       "PTAU/ABETA              0.030044    0.054819    0.245297  \n",
       "Hippocampus/ICV         0.004522    0.005104    0.007087  \n",
       "Entorhinal/ICV          0.002375    0.002731    0.005052  \n",
       "Fusiform/ICV            0.011465    0.012689    0.017211  \n",
       "MidTemp/ICV             0.012891    0.014076    0.019315  \n",
       "Ventricles/ICV          0.024357    0.032912    0.078630  \n",
       "WholeBrain/ICV          0.679397    0.716086    0.819394  \n",
       "DX                      1.500000    2.250000    3.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = []\n",
    "\n",
    "smotenc = SMOTENC(\n",
    "        categorical_features=categorical_features,\n",
    "        sampling_strategy=oversample_dict,\n",
    "        random_state=42\n",
    "    )\n",
    "steps.append(('smotenc', smotenc))\n",
    "\n",
    "rus = RandomUnderSampler(\n",
    "        sampling_strategy=undersample_dict,\n",
    "        random_state=42\n",
    "    )\n",
    "steps.append(('rus', rus))\n",
    "\n",
    "X_train = dataset.drop(columns=['DX'])\n",
    "y_train = dataset['DX']\n",
    "\n",
    "pipeline = Pipeline_imb(steps=steps)\n",
    "X_res, y_res = pipeline.fit_resample(X_train, y_train)\n",
    "sampled = pd.concat([pd.DataFrame(X_res, columns=X_train.columns),\n",
    "                     pd.DataFrame(y_res, columns=['DX'])],\n",
    "                    axis=1)\n",
    "\n",
    "# Distribution after resampling\n",
    "print(\"\\nClass distribution after resampling (count):\")\n",
    "print(sampled['DX'].value_counts())\n",
    "print(\"\\nClass distribution after resampling (percentages):\")\n",
    "print((sampled['DX'].value_counts(normalize=True) * 100).round(2))\n",
    "display(sampled.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled.to_csv(\"../data/sampled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification model choices will be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=False, loss_function='MultiClass'),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(random_state=42, solver='saga', max_iter=2000, class_weight='balanced'))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to select parameters that maximize model performance while reducing the risk of overfitting. Since the dataset is small and the risk of overfitting is high, we will carefully select the best hyperparameters using **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gridsearch(X_train, y_train, classifiers, param_grids, cv=5, scoring='balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Runs GridSearchCV on multiple classifiers with their respective parameter grids.\n",
    "    Ignores classifiers that fail during fitting and continues with the others.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : DataFrame \n",
    "        Training set\n",
    "    classifiers : dict\n",
    "        Dictionary with model names as keys and classifier objects as values.\n",
    "    param_grids : dict\n",
    "        Dictionary with model names as keys and parameter grids as values.\n",
    "    cv : int, default=5\n",
    "        Number of folds for cross-validation.\n",
    "    scoring : str, default='balanced_accuracy'\n",
    "        Scoring metric to optimize.\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    best_models : dict\n",
    "        Dictionary containing best estimator, parameters, and score for each classifier.\n",
    "    \"\"\"    \n",
    "    best_models = {}\n",
    "    errors = {}\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nRunning GridSearch for {name} ...\")\n",
    "        param_grid = param_grids.get(name, {})\n",
    "        \n",
    "        grid = GridSearchCV(\n",
    "            estimator=clf,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            error_score='raise'  # Force the error to catch it\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_models[name] = {\n",
    "                \"best_estimator\": grid.best_estimator_,\n",
    "                \"best_params\": grid.best_params_,\n",
    "                \"best_score\": grid.best_score_\n",
    "            }\n",
    "            print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "            print(f\"Best {scoring}: {grid.best_score_:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Classifier {name} failed: {e}\")\n",
    "            errors[name] = str(e)\n",
    "    \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_gridsearch` takes a *training dataset*, a set of *classifiers*, and their respective *grid_params* and applies **GridSearchCV** to each model. For each classifier, it constructs a grid search with the chosen metric and cross-validation, executes it, prints the best parameters and score, and returns a dictionary that collects the best trained estimator, the optimal parameters, and the corresponding performance for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the parameter grid to compare for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Decision Tree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [6, 5, 4, 3],\n",
    "        'min_samples_split': [2, 8, 16],\n",
    "        'min_samples_leaf': [1, 4, 8],\n",
    "        'max_features': [0.8, 1.0],\n",
    "        'ccp_alpha': [0.0, 0.001, 0.005, 0.01, 0.05]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_depth': [None, 6, 4],\n",
    "        'min_samples_leaf': [2, 4, 8],\n",
    "        'max_features': [0.5, 0.8, 1.0, 'sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_depth': [None, 6, 4],\n",
    "        'min_samples_leaf': [2, 4, 8],\n",
    "        'max_features': [0.5, 0.8, 1.0, 'sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [8, 6, 3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5, 1.0],\n",
    "        'reg_alpha': [0, 1],\n",
    "        'reg_lambda': [0, 1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 15],\n",
    "        'max_depth': [8, 6, 3],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        'reg_alpha': [0, 1],\n",
    "        'reg_lambda': [0, 1]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [50, 75, 100],\n",
    "        'learning_rate': [0.1, 0.05],\n",
    "        'depth': [8, 6, 3],\n",
    "        'l2_leaf_reg': [1, 3, 7],\n",
    "        'border_count': [32, 64, 128],\n",
    "        'bagging_temperature': [0.0, 0.2, 0.5, 1.0],\n",
    "        'random_strength': [0.5, 1, 5]\n",
    "    },\n",
    "    'Multinomial Logistic Regression': {\n",
    "        'logreg__C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'logreg__penalty': ['l1','l2'] \n",
    "    },\n",
    "    'Bagging': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_samples': [0.6, 0.8, 1.0],\n",
    "        'max_features': [0.5, 0.8, 1.0],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the Grid Search (this will take a while)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit`\n",
    "For the dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, we use the **f1_macro** score. These three variables dominate the explanatory variance and produce high but potentially misleading accuracy. f1_macro evaluates the unweighted average of the F1s per class, forcing the grid search to look for hyperparameters that balance precision/recall across all classes (it avoids optimizing a model that only “exploits” sparse features to predict the majority class). We might consider using f1_weighted since it is proportional to support, but it tends to approach accuracy and can mask poor performance on smaller classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best params for Decision Tree: {'ccp_alpha': 0.005, 'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.8, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best f1_macro: 0.9211\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9245\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 2, 'n_estimators': 75}\n",
      "Best f1_macro: 0.9228\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 0.7, 'gamma': 0.5, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 1.0}\n",
      "Best f1_macro: 0.9265\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 8, 'min_child_samples': 20, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "Best f1_macro: 0.9282\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.5, 'border_count': 128, 'depth': 6, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best f1_macro: 0.9260\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 1.0, 'logreg__penalty': 'l1'}\n",
      "Best f1_macro: 0.8737\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': True, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 50}\n",
      "Best f1_macro: 0.9200\n"
     ]
    }
   ],
   "source": [
    "bmc = run_gridsearch(X_train=X_train, y_train=y_train, classifiers=classifiers, param_grids=param_grids, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best params for Decision Tree: {'ccp_alpha': 0.005, 'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.8, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best f1_macro: 0.9066\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9236\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9212\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 0.7, 'gamma': 0.5, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 75, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 1.0}\n",
      "Best f1_macro: 0.9286\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_samples': 20, 'n_estimators': 75, 'num_leaves': 31, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "Best f1_macro: 0.9260\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.2, 'border_count': 128, 'depth': 8, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best f1_macro: 0.9243\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 1.0, 'logreg__penalty': 'l1'}\n",
      "Best f1_macro: 0.8880\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': True, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9151\n"
     ]
    }
   ],
   "source": [
    "bmcs = run_gridsearch(X_train=X_res, y_train=y_res, classifiers=classifiers, param_grids=param_grids, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit`\n",
    "For the dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, we use the **balanced_accuracy** score. By removing the most predictive features, the model must exploit weak signals and complex combinations to directly optimize the accuracy of the various classes. This helps find hyperparameters that improve the classifier's overall performance on the residual feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['CDRSB', 'LDELTOTAL', 'mPACCdigit'], axis=1, inplace=True)\n",
    "X_res.drop(columns=['CDRSB', 'LDELTOTAL', 'mPACCdigit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best params for Decision Tree: {'ccp_alpha': 0.005, 'criterion': 'gini', 'max_depth': 6, 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "Best balanced_accuracy: 0.6752\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.5, 'min_samples_leaf': 8, 'n_estimators': 50}\n",
      "Best balanced_accuracy: 0.7083\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'gini', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 8, 'n_estimators': 75}\n",
      "Best balanced_accuracy: 0.7122\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 75, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7041\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 5, 'n_estimators': 100, 'num_leaves': 31, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7016\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.5, 'border_count': 32, 'depth': 6, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best balanced_accuracy: 0.6989\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 0.1, 'logreg__penalty': 'l1'}\n",
      "Best balanced_accuracy: 0.6936\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': False, 'max_features': 0.8, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "Best balanced_accuracy: 0.6904\n"
     ]
    }
   ],
   "source": [
    "bmcc = run_gridsearch(X_train=X_train, y_train=y_train, classifiers=classifiers, param_grids=param_grids, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n",
      "Best params for Decision Tree: {'ccp_alpha': 0.005, 'criterion': 'entropy', 'max_depth': 5, 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "Best balanced_accuracy: 0.6840\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best balanced_accuracy: 0.7455\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'entropy', 'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best balanced_accuracy: 0.7585\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7610\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 10, 'n_estimators': 75, 'num_leaves': 31, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7585\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.2, 'border_count': 32, 'depth': 8, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best balanced_accuracy: 0.7505\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 1.0, 'logreg__penalty': 'l1'}\n",
      "Best balanced_accuracy: 0.7135\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': False, 'max_features': 0.5, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Best balanced_accuracy: 0.7565\n"
     ]
    }
   ],
   "source": [
    "bmccs = run_gridsearch(X_train=X_res, y_train=y_res, classifiers=classifiers, param_grids=param_grids, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit` we will choose this hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Sampling\n",
    "classifiers_1 = {\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced',\n",
    "        ccp_alpha=0.005, criterion='entropy', max_depth=6,\n",
    "        max_features=0.8, min_samples_leaf=4, min_samples_split=2\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=1.0,\n",
    "        min_samples_leaf=2, n_estimators=100\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=1.0,\n",
    "        min_samples_leaf=2, n_estimators=75\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0,\n",
    "        colsample_bytree=0.7, gamma=0.5, learning_rate=0.1, max_depth=8,\n",
    "        n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=1.0\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, verbose=-1,\n",
    "        colsample_bytree=1.0, learning_rate=0.01, max_depth=8,\n",
    "        min_child_samples=20, n_estimators=100, num_leaves=15,\n",
    "        reg_alpha=0, reg_lambda=0, subsample=0.8\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, verbose=False, loss_function='MultiClass',\n",
    "        bagging_temperature=0.5, border_count=128, depth=6,\n",
    "        iterations=100, l2_leaf_reg=1, learning_rate=0.1, random_strength=0.5\n",
    "    ),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, solver='saga', max_iter=2000, class_weight='balanced',\n",
    "            C=1.0, penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, n_jobs=-1,\n",
    "        bootstrap=True, max_features=0.8, max_samples=0.8, n_estimators=50\n",
    "    )\n",
    "}\n",
    "\n",
    "# Sampling\n",
    "classifiers_2 = {\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced',\n",
    "        ccp_alpha=0.005, criterion='entropy', max_depth=6,\n",
    "        max_features=0.8, min_samples_leaf=4, min_samples_split=2\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=0.5,\n",
    "        min_samples_leaf=2, n_estimators=100\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=1.0,\n",
    "        min_samples_leaf=2, n_estimators=100\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0,\n",
    "        colsample_bytree=0.7, gamma=0.5, learning_rate=0.05, max_depth=6,\n",
    "        n_estimators=75, reg_alpha=0, reg_lambda=1, subsample=1.0\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, verbose=-1,\n",
    "        colsample_bytree=0.7, learning_rate=0.05, max_depth=8,\n",
    "        min_child_samples=20, n_estimators=75, num_leaves=31,\n",
    "        reg_alpha=0, reg_lambda=0, subsample=0.8\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, verbose=False, loss_function='MultiClass',\n",
    "        bagging_temperature=0.2, border_count=128, depth=8,\n",
    "        iterations=100, l2_leaf_reg=1, learning_rate=0.1, random_strength=0.5\n",
    "    ),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, solver='saga', max_iter=2000, class_weight='balanced',\n",
    "            C=1.0, penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, n_jobs=-1,\n",
    "        bootstrap=True, max_features=0.8, max_samples=1.0, n_estimators=100\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit` we will choose this hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Sampling\n",
    "classifiers_3 = {\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced',\n",
    "        ccp_alpha=0.005, criterion='gini', max_depth=6,\n",
    "        max_features=0.8, min_samples_leaf=1, min_samples_split=8\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=6, max_features=0.5,\n",
    "        min_samples_leaf=8, n_estimators=50\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='gini', max_depth=None, max_features=1.0,\n",
    "        min_samples_leaf=8, n_estimators=75\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0,\n",
    "        colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=8,\n",
    "        n_estimators=75, reg_alpha=1, reg_lambda=1, subsample=0.8\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, verbose=-1,\n",
    "        colsample_bytree=0.7, learning_rate=0.1, max_depth=3,\n",
    "        min_child_samples=5, n_estimators=100, num_leaves=31,\n",
    "        reg_alpha=1, reg_lambda=1, subsample=0.8\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, verbose=False, loss_function='MultiClass',\n",
    "        bagging_temperature=0.5, border_count=32, depth=6,\n",
    "        iterations=100, l2_leaf_reg=1, learning_rate=0.1, random_strength=0.5\n",
    "    ),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, solver='saga', max_iter=2000, class_weight='balanced',\n",
    "            C=0.1, penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, n_jobs=-1,\n",
    "        bootstrap=False, max_features=0.8, max_samples=0.6, n_estimators=100\n",
    "    )\n",
    "}\n",
    "\n",
    "# Sampling\n",
    "classifiers_4 = {\n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced',\n",
    "        ccp_alpha=0.005, criterion='entropy', max_depth=5,\n",
    "        max_features=0.8, min_samples_leaf=1, min_samples_split=8\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=0.5,\n",
    "        min_samples_leaf=2, n_estimators=100\n",
    "    ),\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=0.5,\n",
    "        min_samples_leaf=2, n_estimators=100\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0,\n",
    "        colsample_bytree=1.0, gamma=0, learning_rate=0.1, max_depth=8,\n",
    "        n_estimators=100, reg_alpha=1, reg_lambda=1, subsample=0.8\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, verbose=-1,\n",
    "        colsample_bytree=1.0, learning_rate=0.1, max_depth=8,\n",
    "        min_child_samples=10, n_estimators=75, num_leaves=31,\n",
    "        reg_alpha=1, reg_lambda=1, subsample=0.8\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, verbose=False, loss_function='MultiClass',\n",
    "        bagging_temperature=0.2, border_count=32, depth=8,\n",
    "        iterations=100, l2_leaf_reg=1, learning_rate=0.1, random_strength=0.5\n",
    "    ),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, solver='saga', max_iter=2000, class_weight='balanced',\n",
    "            C=1.0, penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, n_jobs=-1,\n",
    "        bootstrap=False, max_features=0.5, max_samples=1.0, n_estimators=100\n",
    "    )\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
