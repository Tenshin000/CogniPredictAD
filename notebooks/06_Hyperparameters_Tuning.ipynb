{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve selected our classification models, but we can't dive right into classification. The next challenge is to optimize the model construction. Since we’re working with a small dataset, the main risk is overfitting. To address this, we’ll apply hyperparameter tuning using **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "\n",
    "from CogniPredictAD.visualization import Visualizer\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from imodels import OptimalTreeClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', 116)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_info_columns', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "Open the training dataset with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>mPACCdigit</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>FDG</th>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>108</td>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>63.63640</td>\n",
       "      <td>-4.84005</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>2.428570</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.833330</td>\n",
       "      <td>2.75000</td>\n",
       "      <td>1.222830</td>\n",
       "      <td>0.040838</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.011311</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.706210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.42702</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.161970</td>\n",
       "      <td>0.020445</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>0.014299</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>0.752850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>300</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>-18.90540</td>\n",
       "      <td>2.300</td>\n",
       "      <td>1.844446</td>\n",
       "      <td>1.248572</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.366668</td>\n",
       "      <td>1.75</td>\n",
       "      <td>3.841666</td>\n",
       "      <td>2.847620</td>\n",
       "      <td>3.033334</td>\n",
       "      <td>2.97</td>\n",
       "      <td>3.166668</td>\n",
       "      <td>3.80000</td>\n",
       "      <td>0.924559</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>0.053417</td>\n",
       "      <td>0.522572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1.5</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>63</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>85.71430</td>\n",
       "      <td>-7.95749</td>\n",
       "      <td>1.850</td>\n",
       "      <td>1.269446</td>\n",
       "      <td>1.166668</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.466668</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.941666</td>\n",
       "      <td>1.294444</td>\n",
       "      <td>1.100002</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.733332</td>\n",
       "      <td>2.10000</td>\n",
       "      <td>1.119130</td>\n",
       "      <td>0.020198</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.013038</td>\n",
       "      <td>0.013942</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.637729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>98</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>7.14286</td>\n",
       "      <td>-1.94841</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.833330</td>\n",
       "      <td>1.25000</td>\n",
       "      <td>1.279034</td>\n",
       "      <td>0.026879</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.012975</td>\n",
       "      <td>0.052196</td>\n",
       "      <td>0.635279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>52</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>18.18180</td>\n",
       "      <td>2.22837</td>\n",
       "      <td>1.500</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.50000</td>\n",
       "      <td>1.416100</td>\n",
       "      <td>0.013555</td>\n",
       "      <td>0.005079</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.014043</td>\n",
       "      <td>0.013729</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.710296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>7.0</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>-9.28099</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>2.428570</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>1.268520</td>\n",
       "      <td>0.080942</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.011582</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>0.711762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>42.85710</td>\n",
       "      <td>-2.30539</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.456170</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.013522</td>\n",
       "      <td>0.013008</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>0.711396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00000</td>\n",
       "      <td>-1.42719</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.222220</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.66667</td>\n",
       "      <td>1.318880</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.002176</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.013614</td>\n",
       "      <td>0.026801</td>\n",
       "      <td>0.663416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.92137</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.444440</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.251688</td>\n",
       "      <td>0.022828</td>\n",
       "      <td>0.004983</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.014308</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.705238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DX  AGE  PTGENDER  PTEDUCAT  APOE4  MMSE  CDRSB  ADAS13  LDELTOTAL  FAQ  \\\n",
       "0      2   77         0        16      1    28    2.5       5          1    0   \n",
       "1      0   59         1        16      1    30    0.0       0         19    0   \n",
       "2      3   77         1        12      2    22    8.0      30          0   25   \n",
       "3      2   82         1        20      0    26    1.5      21          4    0   \n",
       "4      0   83         0        17      0    27    0.0       5         13    3   \n",
       "...   ..  ...       ...       ...    ...   ...    ...     ...        ...  ...   \n",
       "1929   0   72         0        18      1    30    0.0       4         11    0   \n",
       "1930   3   72         0        12      1    26    7.0      29          5   18   \n",
       "1931   0   70         0        17      0    29    0.0      23         10    0   \n",
       "1932   0   84         1        12      0    30    0.5      16         13    0   \n",
       "1933   0   79         0        20      0    29    0.0       6         15    0   \n",
       "\n",
       "      MOCA  TRABSCOR  RAVLT_immediate  RAVLT_learning  RAVLT_perc_forgetting  \\\n",
       "0       24       108               47               5               63.63640   \n",
       "1       30        47               71               2                0.00000   \n",
       "2       17       300               19               1              100.00000   \n",
       "3       24        63               35               1               85.71430   \n",
       "4       25        98               57               7                7.14286   \n",
       "...    ...       ...              ...             ...                    ...   \n",
       "1929    26        52               42               7               18.18180   \n",
       "1930    19        67               34              -1              100.00000   \n",
       "1931    20       300               31               4               42.85710   \n",
       "1932    26        65               27               1               80.00000   \n",
       "1933    28        77               50               3                0.00000   \n",
       "\n",
       "      mPACCdigit  EcogPtMem  EcogPtLang  EcogPtVisspat  EcogPtPlan  \\\n",
       "0       -4.84005      2.250    2.111110       1.000000        1.00   \n",
       "1        5.42702      1.000    1.000000       1.000000        1.00   \n",
       "2      -18.90540      2.300    1.844446       1.248572        1.58   \n",
       "3       -7.95749      1.850    1.269446       1.166668        1.20   \n",
       "4       -1.94841      1.250    1.333330       1.000000        1.00   \n",
       "...          ...        ...         ...            ...         ...   \n",
       "1929     2.22837      1.500    2.333330       1.285710        1.00   \n",
       "1930    -9.28099      1.500    1.000000       1.142860        1.00   \n",
       "1931    -2.30539      1.125    1.111110       1.000000        1.00   \n",
       "1932    -1.42719      2.000    2.000000       2.000000        2.00   \n",
       "1933     1.92137      1.625    1.444440       1.500000        1.00   \n",
       "\n",
       "      EcogPtOrgan  EcogPtDivatt  EcogSPMem  EcogSPLang  EcogSPVisspat  \\\n",
       "0        1.333330          1.00   2.375000    2.111110       2.428570   \n",
       "1        1.000000          1.00   1.000000    1.000000       1.000000   \n",
       "2        1.366668          1.75   3.841666    2.847620       3.033334   \n",
       "3        1.466668          1.60   1.941666    1.294444       1.100002   \n",
       "4        1.333330          1.00   1.375000    1.111110       1.666670   \n",
       "...           ...           ...        ...         ...            ...   \n",
       "1929     2.500000          1.25   1.250000    1.000000       1.200000   \n",
       "1930     1.000000          1.00   3.250000    2.333330       2.428570   \n",
       "1931     1.000000          1.00   1.000000    1.000000       1.000000   \n",
       "1932     1.500000          2.00   1.625000    1.222220       1.285710   \n",
       "1933     1.000000          1.25   1.125000    1.000000       1.000000   \n",
       "\n",
       "      EcogSPPlan  EcogSPOrgan  EcogSPDivatt       FDG  PTAU/ABETA  \\\n",
       "0           2.60     2.833330       2.75000  1.222830    0.040838   \n",
       "1           1.00     1.000000       1.00000  1.161970    0.020445   \n",
       "2           2.97     3.166668       3.80000  0.924559    0.047131   \n",
       "3           1.16     1.733332       2.10000  1.119130    0.020198   \n",
       "4           1.00     1.833330       1.25000  1.279034    0.026879   \n",
       "...          ...          ...           ...       ...         ...   \n",
       "1929        1.00     1.333330       1.50000  1.416100    0.013555   \n",
       "1930        3.20     3.000000       3.50000  1.268520    0.080942   \n",
       "1931        1.00     1.000000       1.00000  1.456170    0.007661   \n",
       "1932        1.25     1.000000       1.66667  1.318880    0.021033   \n",
       "1933        1.00     1.000000       1.00000  1.251688    0.022828   \n",
       "\n",
       "      Hippocampus/ICV  Entorhinal/ICV  Fusiform/ICV  MidTemp/ICV  \\\n",
       "0            0.004524        0.001882      0.012107     0.011311   \n",
       "1            0.004452        0.002756      0.012935     0.014299   \n",
       "2            0.002825        0.001348      0.010049     0.009701   \n",
       "3            0.003736        0.002083      0.013038     0.013942   \n",
       "4            0.004611        0.002170      0.011387     0.012975   \n",
       "...               ...             ...           ...          ...   \n",
       "1929         0.005079        0.003304      0.014043     0.013729   \n",
       "1930         0.004383        0.001691      0.011582     0.011346   \n",
       "1931         0.005042        0.002406      0.013522     0.013008   \n",
       "1932         0.004567        0.002176      0.012360     0.013614   \n",
       "1933         0.004983        0.002936      0.012679     0.014308   \n",
       "\n",
       "      Ventricles/ICV  WholeBrain/ICV  \n",
       "0           0.016977        0.706210  \n",
       "1           0.025614        0.752850  \n",
       "2           0.053417        0.522572  \n",
       "3           0.024176        0.637729  \n",
       "4           0.052196        0.635279  \n",
       "...              ...             ...  \n",
       "1929        0.027992        0.710296  \n",
       "1930        0.022499        0.711762  \n",
       "1931        0.013065        0.711396  \n",
       "1932        0.026801        0.663416  \n",
       "1933        0.020760        0.705238  \n",
       "\n",
       "[1934 rows x 36 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the dataset with pandas\n",
    "dataset = pd.read_csv('../data/train.csv')\n",
    "viz = Visualizer(dataset)\n",
    "dataset.shape\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion about CDRSB, LDELTOTAL, and mPACCdigit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features `CDRSB`, `LDELTOTAL`, and `mPACCdigit` have the highest scores in the **SelectKBest** method with f_classif (**ANOVA F-test**), as highlighted in the *Preprocessing Notebook*.\n",
    "The scores are all above 1500:\n",
    "- `CDRSB`: **1624.9008**\n",
    "- `LDELTOTAL`: **1768.7987**\n",
    "- `mPACCdigit`: **1723.7684**\n",
    "\n",
    "This raises the potential phenomenon of **sparse features**, where a few variables dominate the model, while many others contribute negligibly to the prediction. This can lead to a risk of **local overfitting**, with models that perform very well on the training dataset (here ADNIMERGE), but whose accuracy may decrease on external data.\n",
    "\n",
    "These observations are also supported by the literature: [the study Kauppi et al., 2020 identifies `CDRSB`, `LDELTOTAL`, and `mPACCdigit` among the most important features for predicting disease diagnosis](https://www.medrxiv.org/content/10.1101/2020.11.09.20226746v3.full). This features may be highly predictive in selected cohorts such as ADNI, but their performance could tend to decline in more heterogeneous clinical populations, or with other datasets. \n",
    "\n",
    "In summary, this is not an intrinsic flaw in cognitive tests, but rather a possible **dataset bias**: the observed strong accuracy could reflect the specific structure of ADNI rather than universal predictive validity. \n",
    "\n",
    "**Since we can't determine this, I believe the best course of action is to create a predictive model that includes `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, and a model that ignores them. If these three features prove more efficient at predicting only within this sample, we would still have a predictive model that tends to ignore them and is therefore still useful for prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classification model choices will be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0),\n",
    "    'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=False, loss_function='MultiClass'),\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(random_state=42, solver='saga', max_iter=2000, class_weight='balanced'))\n",
    "    ]),\n",
    "    'Bagging': BaggingClassifier(random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to select parameters that maximize model performance while reducing the risk of overfitting. Since the dataset is small and the risk of overfitting is high, we will carefully select the best hyperparameters using **Grid Search**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def run_gridsearch(train, classifiers, param_grids, cv=5, scoring='balanced_accuracy'):\n",
    "    \"\"\"\n",
    "    Runs GridSearchCV on multiple classifiers with their respective parameter grids.\n",
    "    Ignores classifiers that fail during fitting and continues with the others.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : DataFrame \n",
    "        Training set\n",
    "    classifiers : dict\n",
    "        Dictionary with model names as keys and classifier objects as values.\n",
    "    param_grids : dict\n",
    "        Dictionary with model names as keys and parameter grids as values.\n",
    "    cv : int, default=5\n",
    "        Number of folds for cross-validation.\n",
    "    scoring : str, default='balanced_accuracy'\n",
    "        Scoring metric to optimize.\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    best_models : dict\n",
    "        Dictionary containing best estimator, parameters, and score for each classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Target column\n",
    "    y_train = train['DX']\n",
    "    X_train = train.drop(columns=['DX'])\n",
    "    \n",
    "    best_models = {}\n",
    "    errors = {}\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nRunning GridSearch for {name} ...\")\n",
    "        param_grid = param_grids.get(name, {})\n",
    "        \n",
    "        # Safe n_jobs for XAI models\n",
    "        if 'OptimalTree' in str(type(clf)) or 'ExplainableBoosting' in str(type(clf)):\n",
    "            n_jobs_grid = 1\n",
    "        else:\n",
    "            n_jobs_grid = -1\n",
    "        \n",
    "        grid = GridSearchCV(\n",
    "            estimator=clf,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=n_jobs_grid,\n",
    "            verbose=1,\n",
    "            error_score='raise'  # forza l'errore per catturarlo\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            grid.fit(X_train, y_train)\n",
    "            best_models[name] = {\n",
    "                \"best_estimator\": grid.best_estimator_,\n",
    "                \"best_params\": grid.best_params_,\n",
    "                \"best_score\": grid.best_score_\n",
    "            }\n",
    "            print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "            print(f\"Best {scoring}: {grid.best_score_:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Classifier {name} failed: {e}\")\n",
    "            errors[name] = str(e)\n",
    "    \n",
    "    return best_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`run_gridsearch` takes a *training dataset*, a set of *classifiers*, and their respective *grid_params* and applies **GridSearchCV** to each model. It uses the `DX` column as the target variable and all other columns as features. For each classifier, it constructs a grid search with the chosen metric and cross-validation, executes it, prints the best parameters and score, and returns a dictionary that collects the best trained estimator, the optimal parameters, and the corresponding performance for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the parameter grid to compare for the standard and XAI classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Decision Tree': {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [6, 5, 4, 3],\n",
    "        'min_samples_split': [2, 8, 16],\n",
    "        'min_samples_leaf': [1, 4, 8],\n",
    "        'max_features': [0.8, 1.0]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_depth': [None, 6, 4],\n",
    "        'min_samples_leaf': [2, 4, 8],\n",
    "        'max_features': [0.5, 0.8, 1.0, 'sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'Extra Trees': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_depth': [None, 6, 4],\n",
    "        'min_samples_leaf': [2, 4, 8],\n",
    "        'max_features': [0.5, 0.8, 1.0, 'sqrt', 'log2'],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [8, 6, 3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5, 1.0],\n",
    "        'reg_alpha': [0, 1],\n",
    "        'reg_lambda': [0, 1]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 15],\n",
    "        'max_depth': [8, 6, 3],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "        'reg_alpha': [0, 1],\n",
    "        'reg_lambda': [0, 1]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [50, 75, 100],\n",
    "        'learning_rate': [0.1, 0.05],\n",
    "        'depth': [8, 6, 3],\n",
    "        'l2_leaf_reg': [1, 3, 7],\n",
    "        'border_count': [32, 64, 128],\n",
    "        'bagging_temperature': [0.0, 0.2, 0.5, 1.0],\n",
    "        'random_strength': [0.5, 1, 5]\n",
    "    },\n",
    "    'Multinomial Logistic Regression': {\n",
    "        'logreg__C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'logreg__penalty': ['l1','l2'] \n",
    "    },\n",
    "    'Bagging': {\n",
    "        'n_estimators': [50, 75, 100],\n",
    "        'max_samples': [0.6, 0.8, 1.0],\n",
    "        'max_features': [0.5, 0.8, 1.0],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the Grid Search (this will take a while)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit`\n",
    "For the dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, we use the **f1_macro** score. These three variables dominate the explanatory variance and produce high but potentially misleading accuracy. f1_macro evaluates the unweighted average of the F1s per class, forcing the grid search to look for hyperparameters that balance precision/recall across all classes (it avoids optimizing a model that only “exploits” sparse features to predict the majority class). We might consider using f1_weighted since it is proportional to support, but it tends to approach accuracy and can mask poor performance on smaller classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best params for Decision Tree: {'max_depth': None, 'max_features': 0.8, 'min_samples_leaf': 10, 'min_samples_split': 2}\n",
      "Best f1_macro: 0.9037\n",
      "\n",
      "Running GridSearch for OptimalTree ...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best params for OptimalTree: {'feature_exchange': True, 'look_ahead': True, 'regularization': 0.0}\n",
      "Best f1_macro: 0.8683\n",
      "\n",
      "Running GridSearch for ExplainableBoosting ...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params for ExplainableBoosting: {'learning_rate': 0.01, 'max_bins': 128}\n",
      "Best f1_macro: 0.9153\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9247\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 2, 'n_estimators': 75}\n",
      "Best f1_macro: 0.9232\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 0.7, 'gamma': 1.0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 1.0}\n",
      "Best f1_macro: 0.9269\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 8, 'min_child_samples': 20, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best f1_macro: 0.9248\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.0, 'border_count': 64, 'depth': 8, 'iterations': 75, 'l2_leaf_reg': 3, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best f1_macro: 0.9252\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 1.0, 'logreg__penalty': 'l1'}\n",
      "Best f1_macro: 0.8722\n",
      "\n",
      "Running GridSearch for KNN ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best params for KNN: {'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "Best f1_macro: 0.7018\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': False, 'max_features': 1.0, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "Best f1_macro: 0.9213\n"
     ]
    }
   ],
   "source": [
    "bmc = run_gridsearch(train=dataset, classifiers=classifiers, param_grids=param_grids, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit`\n",
    "For the dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit`, we use the **balanced_accuracy** score. By removing the most predictive features, the model must exploit weak signals and complex combinations to directly optimize the accuracy of the various classes. This helps find hyperparameters that improve the classifier's overall performance on the residual feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['CDRSB', 'LDELTOTAL', 'mPACCdigit'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Decision Tree: {'max_depth': 4, 'max_features': 1.0, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Best balanced_accuracy: 0.6649\n",
      "\n",
      "Running GridSearch for OptimalTree ...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best params for OptimalTree: {'feature_exchange': True, 'look_ahead': True, 'regularization': 0.0}\n",
      "Best balanced_accuracy: 0.6131\n",
      "\n",
      "Running GridSearch for ExplainableBoosting ...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params for ExplainableBoosting: {'learning_rate': 0.005, 'max_bins': 128}\n",
      "Best balanced_accuracy: 0.7006\n",
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Random Forest: {'criterion': 'entropy', 'max_depth': 6, 'max_features': 0.5, 'min_samples_leaf': 2, 'n_estimators': 50}\n",
      "Best balanced_accuracy: 0.7166\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "Best params for Extra Trees: {'criterion': 'entropy', 'max_depth': None, 'max_features': 1.0, 'min_samples_leaf': 8, 'n_estimators': 50}\n",
      "Best balanced_accuracy: 0.7168\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 2592 candidates, totalling 12960 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 1.0, 'gamma': 1.0, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 100, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7114\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 3888 candidates, totalling 19440 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 5, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best balanced_accuracy: 0.7100\n",
      "\n",
      "Running GridSearch for CatBoost ...\n",
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n",
      "Best params for CatBoost: {'bagging_temperature': 0.0, 'border_count': 32, 'depth': 6, 'iterations': 100, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'random_strength': 0.5}\n",
      "Best balanced_accuracy: 0.7053\n",
      "\n",
      "Running GridSearch for Multinomial Logistic Regression ...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for Multinomial Logistic Regression: {'logreg__C': 1.0, 'logreg__penalty': 'l1'}\n",
      "Best balanced_accuracy: 0.6994\n",
      "\n",
      "Running GridSearch for KNN ...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best params for KNN: {'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance'}\n",
      "Best balanced_accuracy: 0.6412\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best params for Bagging: {'bootstrap': False, 'max_features': 0.8, 'max_samples': 0.6, 'n_estimators': 100}\n",
      "Best balanced_accuracy: 0.6947\n"
     ]
    }
   ],
   "source": [
    "bmcc = run_gridsearch(train=dataset, classifiers=classifiers, param_grids=param_grids, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **Dataset with `CDRSB`, `LDELTOTAL`, and `mPACCdigit` we will choose this hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_1 = { \n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, \n",
    "        class_weight='balanced',\n",
    "        max_depth=None,\n",
    "        max_features=0.8,\n",
    "        min_samples_leaf=10,\n",
    "        min_samples_split=2\n",
    "    ),\n",
    "\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1,\n",
    "        criterion='entropy',\n",
    "        max_depth=None,\n",
    "        max_features=1.0,\n",
    "        min_samples_leaf=2,\n",
    "        n_estimators=100\n",
    "    ),\n",
    "\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1,\n",
    "        criterion='entropy',\n",
    "        max_depth=None,\n",
    "        max_features=1.0,\n",
    "        min_samples_leaf=2,\n",
    "        n_estimators=75\n",
    "    ),\n",
    "\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='mlogloss', \n",
    "        verbosity=0,\n",
    "        colsample_bytree=0.7,\n",
    "        gamma=1.0,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        n_estimators=50,\n",
    "        reg_alpha=1,\n",
    "        reg_lambda=0,\n",
    "        subsample=1.0\n",
    "    ),\n",
    "\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, \n",
    "        verbose=-1,\n",
    "        colsample_bytree=1.0,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=8,\n",
    "        min_child_samples=20,\n",
    "        n_estimators=100,\n",
    "        num_leaves=15,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=1,\n",
    "        subsample=0.8\n",
    "    ),\n",
    "\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, \n",
    "        verbose=False, \n",
    "        loss_function='MultiClass',\n",
    "        bagging_temperature=0.0,\n",
    "        border_count=64,\n",
    "        depth=8,\n",
    "        iterations=75,\n",
    "        l2_leaf_reg=3,\n",
    "        learning_rate=0.1,\n",
    "        random_strength=0.5\n",
    "    ),\n",
    "\n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, \n",
    "            solver='saga', \n",
    "            max_iter=2000, \n",
    "            class_weight='balanced',\n",
    "            C=1.0,\n",
    "            penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, \n",
    "        n_jobs=-1,\n",
    "        bootstrap=False,\n",
    "        max_features=1.0,\n",
    "        max_samples=0.6,\n",
    "        n_estimators=100\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **Dataset without `CDRSB`, `LDELTOTAL`, and `mPACCdigit` we will choose this hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_2 = { \n",
    "    'Decision Tree': DecisionTreeClassifier(\n",
    "        random_state=42, class_weight='balanced',\n",
    "        max_depth=4, max_features=1.0,\n",
    "        min_samples_leaf=1, min_samples_split=2\n",
    "    ),\n",
    "\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=6, max_features=0.5,\n",
    "        min_samples_leaf=2, n_estimators=50\n",
    "    ),\n",
    "\n",
    "    'Extra Trees': ExtraTreesClassifier(\n",
    "        random_state=42, class_weight='balanced', n_jobs=-1,\n",
    "        criterion='entropy', max_depth=None, max_features=1.0,\n",
    "        min_samples_leaf=8, n_estimators=50\n",
    "    ),\n",
    "\n",
    "    'XGBoost': XGBClassifier(\n",
    "        random_state=42, use_label_encoder=False, eval_metric='mlogloss', verbosity=0,\n",
    "        colsample_bytree=1.0, gamma=1.0, learning_rate=0.1,\n",
    "        max_depth=8, n_estimators=100, reg_alpha=0, reg_lambda=1,\n",
    "        subsample=0.8\n",
    "    ),\n",
    "\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        random_state=42, verbose=-1,\n",
    "        colsample_bytree=0.7, learning_rate=0.1, max_depth=8,\n",
    "        min_child_samples=5, n_estimators=100, num_leaves=15,\n",
    "        reg_alpha=1, reg_lambda=1, subsample=0.8\n",
    "    ),\n",
    "\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        random_state=42, verbose=False, loss_function='MultiClass',\n",
    "        bagging_temperature=0.0, border_count=32, depth=6,\n",
    "        iterations=100, l2_leaf_reg=1, learning_rate=0.1,\n",
    "        random_strength=0.5\n",
    "    ),\n",
    "    \n",
    "    'Multinomial Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(\n",
    "            random_state=42, solver='saga', max_iter=2000, class_weight='balanced',\n",
    "            C=1.0, penalty='l1'\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    'Bagging': BaggingClassifier(\n",
    "        random_state=42, n_jobs=-1,\n",
    "        bootstrap=False, max_features=0.8,\n",
    "        max_samples=0.6, n_estimators=100\n",
    "    )\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CogniPredictAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
