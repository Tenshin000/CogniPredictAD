{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing** is the initial phase of a Machine Learning project in which raw data is transformed into a cleaner, more consistent format suitable for analysis or model training. We've already prepared the data for splitting, avoiding inconsistencies and data leakage. Now let's do the rest of the Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import warnings\n",
    "\n",
    "from CogniPredictAD.preprocessing import ADNICleaner, ADNITransformator, OutlierDetector\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"No artists with labels found to put in legend.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Duplicate values are leading to incorrect results\")\n",
    "\n",
    "pd.set_option('display.max_rows', 116)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_info_columns', 50) \n",
    "\n",
    "train = pd.read_csv(\"../data/pretrain.csv\")\n",
    "test = pd.read_csv(\"../data/pretest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning** is the first step of Preprocessing and it is used for detecting and correcting or removing errors and inconsistencies in the dataset to improve data quality and model performance. Clean data ensures that the model learns from accurate, relevant, and reliable inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cleaner = ADNICleaner(train)\n",
    "test_cleaner = ADNICleaner(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADNICleaner** is a utility class for *Data Cleaning* and *Preprocessing* of the ***ADNIMERGE*** dataset. It stores a DataFrame and provides a collection of data-cleaning utilities tailored for baseline visit selection, missing-value handling, column consolidation, and dataset simplification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the percentage of having Null values in each column and prints in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of NULL values per column:\n",
      "ABETA                    50.10%\n",
      "TAU                      50.10%\n",
      "PTAU                     50.10%\n",
      "FDG                      37.69%\n",
      "EcogSPOrgan              34.28%\n",
      "EcogSPVisspat            33.25%\n",
      "EcogSPDivatt             33.20%\n",
      "EcogPtOrgan              32.83%\n",
      "EcogSPPlan               32.83%\n",
      "EcogSPMem                32.63%\n",
      "MOCA                     32.57%\n",
      "EcogSPTotal              32.57%\n",
      "EcogSPLang               32.52%\n",
      "EcogPtVisspat            32.37%\n",
      "EcogPtDivatt             32.16%\n",
      "EcogPtLang               31.95%\n",
      "EcogPtPlan               31.95%\n",
      "EcogPtMem                31.90%\n",
      "EcogPtTotal              31.90%\n",
      "MidTemp                  13.08%\n",
      "Fusiform                 13.08%\n",
      "Entorhinal               13.08%\n",
      "Hippocampus              10.75%\n",
      "APOE4                     8.79%\n",
      "Ventricles                5.69%\n",
      "WholeBrain                4.55%\n",
      "ICV                       3.05%\n",
      "TRABSCOR                  1.50%\n",
      "FAQ                       0.93%\n",
      "ADAS13                    0.47%\n",
      "RAVLT_perc_forgetting     0.41%\n",
      "ADAS11                    0.31%\n",
      "RAVLT_forgetting          0.21%\n",
      "RAVLT_learning            0.16%\n",
      "ADASQ4                    0.16%\n",
      "RAVLT_immediate           0.16%\n",
      "AGE                       0.10%\n",
      "LDELTOTAL                 0.10%\n",
      "MMSE                      0.05%\n",
      "mPACCtrailsB              0.05%\n",
      "mPACCdigit                0.05%\n",
      "PTEDUCAT                  0.00%\n",
      "CDRSB                     0.00%\n",
      "PTGENDER                  0.00%\n",
      "DX                        0.00%\n",
      "PTDEMOGROUP               0.00%\n",
      "MARRIED                   0.00%\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentage of NULL values per column:\")\n",
    "null_percent = (train.isna().sum() / len(train) * 100).sort_values(ascending=False)\n",
    "print(null_percent.apply(lambda x: f\"{x:.2f}%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the **K-Nearest Neighbors** (KNN) to impute the NULL values, but there is a problem. The “pure” NumPy np.int64 type cannot contain NaNs. This is because NaN is a special value of the floating point type (float), not integers. sklearn.impute.KNNImputer works on NumPy arrays (np.float64 or real np.int64). Pandas' pd.Int64Dtype() is not an np.int64, it is an extension (ExtensionDtype) that internally represents the data as an array of ints + a Boolean mask for the NAs. When you pass a DataFrame with nullable Int64s to scikit-learn, it tries to convert it to NumPy, and since pd.NA is not a number, it converts it to an object, and KNNImputer cannot work on objects. So we take the columns that are integers, leave them floats, run KNN through them, and then convert them to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with only integer values:\n",
      "['APOE4', 'ADASQ4', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning', 'RAVLT_forgetting', 'LDELTOTAL', 'TRABSCOR', 'FAQ', 'WholeBrain', 'Entorhinal', 'Fusiform', 'MidTemp', 'ICV', 'MOCA', 'PTGENDER', 'PTEDUCAT', 'PTDEMOGROUP', 'MARRIED']\n"
     ]
    }
   ],
   "source": [
    "int_columns = [\n",
    "    col for col in train.select_dtypes(include=['float']).columns\n",
    "    if (train[col].dropna() % 1 == 0).all()\n",
    "]\n",
    "\n",
    "# We also add the columns already of type int\n",
    "int_columns += list(train.select_dtypes(include=['int']).columns)\n",
    "\n",
    "print(\"Columns with only integer values:\")\n",
    "print(int_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1934 entries, 0 to 1933\n",
      "Data columns (total 47 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   DX                     1934 non-null   object \n",
      " 1   AGE                    1932 non-null   float64\n",
      " 2   PTGENDER               1934 non-null   int64  \n",
      " 3   PTEDUCAT               1934 non-null   int64  \n",
      " 4   APOE4                  1764 non-null   float64\n",
      " 5   CDRSB                  1934 non-null   float64\n",
      " 6   ADAS11                 1928 non-null   float64\n",
      " 7   ADAS13                 1925 non-null   float64\n",
      " 8   ADASQ4                 1931 non-null   float64\n",
      " 9   MMSE                   1933 non-null   float64\n",
      " 10  RAVLT_immediate        1931 non-null   float64\n",
      " 11  RAVLT_learning         1931 non-null   float64\n",
      " 12  RAVLT_forgetting       1930 non-null   float64\n",
      " 13  RAVLT_perc_forgetting  1926 non-null   float64\n",
      " 14  LDELTOTAL              1932 non-null   float64\n",
      " 15  TRABSCOR               1905 non-null   float64\n",
      " 16  FAQ                    1916 non-null   float64\n",
      " 17  mPACCdigit             1933 non-null   float64\n",
      " 18  mPACCtrailsB           1933 non-null   float64\n",
      " 19  Ventricles             1824 non-null   float64\n",
      " 20  Hippocampus            1726 non-null   float64\n",
      " 21  WholeBrain             1846 non-null   float64\n",
      " 22  Entorhinal             1681 non-null   float64\n",
      " 23  Fusiform               1681 non-null   float64\n",
      " 24  MidTemp                1681 non-null   float64\n",
      " 25  ICV                    1875 non-null   float64\n",
      " 26  MOCA                   1304 non-null   float64\n",
      " 27  EcogPtMem              1317 non-null   float64\n",
      " 28  EcogPtLang             1316 non-null   float64\n",
      " 29  EcogPtVisspat          1308 non-null   float64\n",
      " 30  EcogPtPlan             1316 non-null   float64\n",
      " 31  EcogPtOrgan            1299 non-null   float64\n",
      " 32  EcogPtDivatt           1312 non-null   float64\n",
      " 33  EcogPtTotal            1317 non-null   float64\n",
      " 34  EcogSPMem              1303 non-null   float64\n",
      " 35  EcogSPLang             1305 non-null   float64\n",
      " 36  EcogSPVisspat          1291 non-null   float64\n",
      " 37  EcogSPPlan             1299 non-null   float64\n",
      " 38  EcogSPOrgan            1271 non-null   float64\n",
      " 39  EcogSPDivatt           1292 non-null   float64\n",
      " 40  EcogSPTotal            1304 non-null   float64\n",
      " 41  ABETA                  965 non-null    float64\n",
      " 42  TAU                    965 non-null    float64\n",
      " 43  PTAU                   965 non-null    float64\n",
      " 44  FDG                    1205 non-null   float64\n",
      " 45  PTDEMOGROUP            1934 non-null   int64  \n",
      " 46  MARRIED                1934 non-null   int64  \n",
      "dtypes: float64(42), int64(4), object(1)\n",
      "memory usage: 710.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a K-Nearest Neighbors imputer to replace NULL values with the most likely values. Let's set k = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors imputation applied...\n"
     ]
    }
   ],
   "source": [
    "# Utility function: impute a group using a KNN fitted on train and applied to test\n",
    "def knn_impute_group(train_df: pd.DataFrame, test_df: pd.DataFrame, cols: list, n_neighbors: int = 5):\n",
    "    \"\"\"\n",
    "    Fit a KNNImputer on train_df[cols] after scaling (train mean/std),\n",
    "    transform both train and test for these cols, and replace the values in-place.\n",
    "\n",
    "    Steps:\n",
    "      1. Keep only columns present in train_df.\n",
    "      2. Coerce non-numeric values to NaN (safe conversion).\n",
    "      3. Compute train mean/std ignoring NaNs (nan-aware).\n",
    "      4. Standardize train and test using train statistics.\n",
    "      5. Fit KNNImputer on scaled train and transform both train and test.\n",
    "      6. Inverse-scale imputed results back to the original scale.\n",
    "      7. Put imputed columns back into the original DataFrames preserving indices.\n",
    "\n",
    "    Returns:\n",
    "      (knn_imputer_object, means_array, stds_array) or (None, None, None) if no cols.\n",
    "    \"\"\"\n",
    "    # keep only columns that actually exist in train_df\n",
    "    cols = [c for c in cols if c in train_df.columns]\n",
    "    if len(cols) == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    # Coerce to numeric: non-convertible values become NaN\n",
    "    # Use copy() to avoid modifying original slices unexpectedly\n",
    "    train_block = train_df[cols].apply(pd.to_numeric, errors=\"coerce\").astype(float).copy()\n",
    "    test_block = test_df[cols].apply(pd.to_numeric, errors=\"coerce\").astype(float).copy()\n",
    "\n",
    "    # Compute mean/std on train, ignoring NaNs\n",
    "    means = np.nanmean(train_block.values, axis=0)\n",
    "    stds = np.nanstd(train_block.values, axis=0)\n",
    "    # Prevent division by zero for constant columns\n",
    "    stds[stds == 0] = 1.0\n",
    "\n",
    "    # Standardize (nan-aware)\n",
    "    train_scaled = (train_block.values - means) / stds\n",
    "    test_scaled = (test_block.values - means) / stds\n",
    "\n",
    "    # Fit KNNImputer on scaled train and transform both train and test\n",
    "    knn = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_train_scaled = knn.fit_transform(train_scaled)\n",
    "    imputed_test_scaled = knn.transform(test_scaled)\n",
    "\n",
    "    # Inverse scale back to original units\n",
    "    imputed_train = imputed_train_scaled * stds + means\n",
    "    imputed_test = imputed_test_scaled * stds + means\n",
    "\n",
    "    # Put the imputed arrays back into the original DataFrames (preserve indices)\n",
    "    train_df.loc[:, cols] = pd.DataFrame(imputed_train, index=train_df.index, columns=cols)\n",
    "    test_df.loc[:, cols] = pd.DataFrame(imputed_test, index=test_df.index, columns=cols)\n",
    "\n",
    "    return knn, means, stds\n",
    "\n",
    "cols_to_impute = [c for c in train.columns if c != \"DX\"]\n",
    "knn_obj, means_used, stds_used = knn_impute_group(train, test, cols_to_impute)\n",
    "\n",
    "print(\"K-Nearest Neighbors imputation applied...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the int columns back to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Columns\n",
      "Column 'APOE4' converted using round-half-up rounding.\n",
      "Column 'ADASQ4' converted using round-half-up rounding.\n",
      "Column 'MMSE' converted using round-half-up rounding.\n",
      "Column 'RAVLT_immediate' converted using round-half-up rounding.\n",
      "Column 'RAVLT_learning' converted using round-half-up rounding.\n",
      "Column 'RAVLT_forgetting' converted using round-half-up rounding.\n",
      "Column 'LDELTOTAL' converted using round-half-up rounding.\n",
      "Column 'TRABSCOR' converted using round-half-up rounding.\n",
      "Column 'FAQ' converted using round-half-up rounding.\n",
      "Column 'WholeBrain' converted using round-half-up rounding.\n",
      "Column 'Entorhinal' converted using round-half-up rounding.\n",
      "Column 'Fusiform' converted using round-half-up rounding.\n",
      "Column 'MidTemp' converted using round-half-up rounding.\n",
      "Column 'ICV' converted using round-half-up rounding.\n",
      "Column 'MOCA' converted using round-half-up rounding.\n",
      "Column 'PTGENDER' converted using round-half-up rounding.\n",
      "Column 'PTEDUCAT' converted using round-half-up rounding.\n",
      "Column 'PTDEMOGROUP' converted using round-half-up rounding.\n",
      "Column 'MARRIED' converted using round-half-up rounding.\n",
      "\n",
      "Test Columns\n",
      "Column 'APOE4' converted using round-half-up rounding.\n",
      "Column 'ADASQ4' converted using round-half-up rounding.\n",
      "Column 'MMSE' converted using round-half-up rounding.\n",
      "Column 'RAVLT_immediate' converted using round-half-up rounding.\n",
      "Column 'RAVLT_learning' converted using round-half-up rounding.\n",
      "Column 'RAVLT_forgetting' converted using round-half-up rounding.\n",
      "Column 'LDELTOTAL' converted using round-half-up rounding.\n",
      "Column 'TRABSCOR' converted using round-half-up rounding.\n",
      "Column 'FAQ' converted using round-half-up rounding.\n",
      "Column 'WholeBrain' converted using round-half-up rounding.\n",
      "Column 'Entorhinal' converted using round-half-up rounding.\n",
      "Column 'Fusiform' converted using round-half-up rounding.\n",
      "Column 'MidTemp' converted using round-half-up rounding.\n",
      "Column 'ICV' converted using round-half-up rounding.\n",
      "Column 'MOCA' converted using round-half-up rounding.\n",
      "Column 'PTGENDER' converted using round-half-up rounding.\n",
      "Column 'PTEDUCAT' converted using round-half-up rounding.\n",
      "Column 'PTDEMOGROUP' converted using round-half-up rounding.\n",
      "Column 'MARRIED' converted using round-half-up rounding.\n",
      "\n",
      "Conversion completed...\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Columns\")\n",
    "# Convert \"train\" columns back to int64\n",
    "for col in int_columns:\n",
    "    train = train_cleaner.convert_float_to_int(column=col, method=\"round\", dataset=train)\n",
    "\n",
    "print(\"\\nTest Columns\")\n",
    "# Convert \"test\" columns back to int64\n",
    "for col in int_columns:\n",
    "    test = test_cleaner.convert_float_to_int(column=col, method=\"round\", dataset=test)\n",
    "\n",
    "print(\"\\nConversion completed...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1934 entries, 0 to 1933\n",
      "Data columns (total 47 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   DX                     1934 non-null   object \n",
      " 1   AGE                    1934 non-null   float64\n",
      " 2   PTGENDER               1934 non-null   int64  \n",
      " 3   PTEDUCAT               1934 non-null   int64  \n",
      " 4   APOE4                  1934 non-null   int64  \n",
      " 5   CDRSB                  1934 non-null   float64\n",
      " 6   ADAS11                 1934 non-null   float64\n",
      " 7   ADAS13                 1934 non-null   float64\n",
      " 8   ADASQ4                 1934 non-null   int64  \n",
      " 9   MMSE                   1934 non-null   int64  \n",
      " 10  RAVLT_immediate        1934 non-null   int64  \n",
      " 11  RAVLT_learning         1934 non-null   int64  \n",
      " 12  RAVLT_forgetting       1934 non-null   int64  \n",
      " 13  RAVLT_perc_forgetting  1934 non-null   float64\n",
      " 14  LDELTOTAL              1934 non-null   int64  \n",
      " 15  TRABSCOR               1934 non-null   int64  \n",
      " 16  FAQ                    1934 non-null   int64  \n",
      " 17  mPACCdigit             1934 non-null   float64\n",
      " 18  mPACCtrailsB           1934 non-null   float64\n",
      " 19  Ventricles             1934 non-null   float64\n",
      " 20  Hippocampus            1934 non-null   float64\n",
      " 21  WholeBrain             1934 non-null   int64  \n",
      " 22  Entorhinal             1934 non-null   int64  \n",
      " 23  Fusiform               1934 non-null   int64  \n",
      " 24  MidTemp                1934 non-null   int64  \n",
      " 25  ICV                    1934 non-null   int64  \n",
      " 26  MOCA                   1934 non-null   int64  \n",
      " 27  EcogPtMem              1934 non-null   float64\n",
      " 28  EcogPtLang             1934 non-null   float64\n",
      " 29  EcogPtVisspat          1934 non-null   float64\n",
      " 30  EcogPtPlan             1934 non-null   float64\n",
      " 31  EcogPtOrgan            1934 non-null   float64\n",
      " 32  EcogPtDivatt           1934 non-null   float64\n",
      " 33  EcogPtTotal            1934 non-null   float64\n",
      " 34  EcogSPMem              1934 non-null   float64\n",
      " 35  EcogSPLang             1934 non-null   float64\n",
      " 36  EcogSPVisspat          1934 non-null   float64\n",
      " 37  EcogSPPlan             1934 non-null   float64\n",
      " 38  EcogSPOrgan            1934 non-null   float64\n",
      " 39  EcogSPDivatt           1934 non-null   float64\n",
      " 40  EcogSPTotal            1934 non-null   float64\n",
      " 41  ABETA                  1934 non-null   float64\n",
      " 42  TAU                    1934 non-null   float64\n",
      " 43  PTAU                   1934 non-null   float64\n",
      " 44  FDG                    1934 non-null   float64\n",
      " 45  PTDEMOGROUP            1934 non-null   int64  \n",
      " 46  MARRIED                1934 non-null   int64  \n",
      "dtypes: float64(27), int64(19), object(1)\n",
      "memory usage: 710.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that **AGE** and notice that it is not an integer, but a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE\n",
      "73.5    18\n",
      "76.4    17\n",
      "75.8    15\n",
      "70.6    15\n",
      "71.4    15\n",
      "        ..\n",
      "61.5     1\n",
      "88.8     1\n",
      "57.5     1\n",
      "87.9     1\n",
      "50.4     1\n",
      "Name: count, Length: 340, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['AGE'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's best to change `AGE` from float to int because decimals don't add significant value to the information and make the analysis less clear. Age is typically treated as an integer in clinical and statistical practice, so keeping it as a float can introduce unnecessarily fragmented categories in counts or graphs, increasing data noise. Additionally, using integer values makes it easier to read and classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Column\n",
      "Column 'AGE' converted using floor rounding.\n",
      "\n",
      "Test Column\n",
      "Column 'AGE' converted using floor rounding.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Column\")\n",
    "train = train_cleaner.convert_float_to_int(\"AGE\", method=\"floor\", dataset=train)\n",
    "print(\"\\nTest Column\")\n",
    "test = test_cleaner.convert_float_to_int(\"AGE\", method=\"floor\", dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check `ADAS11` and `ADAS13` that should have integer scores but we notice that they are floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAS11\n",
      "6.000     100\n",
      "9.000      97\n",
      "5.000      97\n",
      "7.000      87\n",
      "4.000      86\n",
      "8.000      76\n",
      "3.000      76\n",
      "10.000     69\n",
      "11.000     62\n",
      "14.000     47\n",
      "7.330      42\n",
      "12.000     40\n",
      "2.000      36\n",
      "6.670      35\n",
      "13.000     31\n",
      "5.670      31\n",
      "4.330      29\n",
      "5.330      29\n",
      "10.670     28\n",
      "3.330      28\n",
      "15.000     27\n",
      "6.330      27\n",
      "8.330      26\n",
      "7.670      25\n",
      "9.670      25\n",
      "3.670      25\n",
      "11.330     24\n",
      "16.000     24\n",
      "8.670      24\n",
      "12.670     23\n",
      "17.000     22\n",
      "13.330     21\n",
      "13.670     21\n",
      "9.330      20\n",
      "2.330      20\n",
      "18.000     20\n",
      "14.330     19\n",
      "19.000     17\n",
      "22.000     17\n",
      "23.000     16\n",
      "11.670     16\n",
      "21.000     14\n",
      "1.670      14\n",
      "2.670      14\n",
      "1.000      14\n",
      "4.670      13\n",
      "16.330     13\n",
      "18.330     13\n",
      "17.670     13\n",
      "14.670     12\n",
      "16.670     12\n",
      "10.330     12\n",
      "15.670     11\n",
      "12.330     11\n",
      "20.000     11\n",
      "15.330     10\n",
      "17.330      9\n",
      "25.000      8\n",
      "21.330      8\n",
      "19.670      8\n",
      "20.330      7\n",
      "1.330       7\n",
      "28.000      7\n",
      "24.000      7\n",
      "18.670      7\n",
      "26.670      6\n",
      "22.330      6\n",
      "21.670      5\n",
      "19.330      5\n",
      "26.000      4\n",
      "29.000      4\n",
      "27.000      4\n",
      "0.670       4\n",
      "37.000      4\n",
      "31.000      3\n",
      "24.670      3\n",
      "0.000       3\n",
      "25.330      3\n",
      "32.330      2\n",
      "29.330      2\n",
      "23.330      2\n",
      "35.000      2\n",
      "26.330      2\n",
      "20.670      2\n",
      "35.330      2\n",
      "22.670      2\n",
      "36.000      2\n",
      "23.670      1\n",
      "17.468      1\n",
      "33.000      1\n",
      "40.000      1\n",
      "34.330      1\n",
      "34.670      1\n",
      "4.266       1\n",
      "27.670      1\n",
      "28.670      1\n",
      "42.670      1\n",
      "30.330      1\n",
      "8.334       1\n",
      "28.330      1\n",
      "3.932       1\n",
      "25.670      1\n",
      "38.000      1\n",
      "24.330      1\n",
      "5.600       1\n",
      "36.330      1\n",
      "27.330      1\n",
      "32.000      1\n",
      "6.800       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "ADAS13\n",
      "9.00     68\n",
      "8.00     67\n",
      "10.00    63\n",
      "12.00    60\n",
      "11.00    56\n",
      "         ..\n",
      "48.33     1\n",
      "9.80      1\n",
      "47.00     1\n",
      "52.00     1\n",
      "50.00     1\n",
      "Name: count, Length: 149, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['ADAS11'].value_counts())\n",
    "print(\"\\n\")\n",
    "print(train['ADAS13'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of float values in these columns is likely the result of a combination of missing data imputation, Pandas data processing, and data collection practices. However, we should keep these values as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Columns\n",
      "Column 'ADAS11' converted using round-half-up rounding.\n",
      "Column 'ADAS13' converted using round-half-up rounding.\n",
      "\n",
      "Test Columns\n",
      "Column 'ADAS11' converted using round-half-up rounding.\n",
      "Column 'ADAS13' converted using round-half-up rounding.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Columns\")\n",
    "train = train_cleaner.convert_float_to_int(\"ADAS11\", method=\"round\", dataset=train)\n",
    "train = train_cleaner.convert_float_to_int(\"ADAS13\", method=\"round\", dataset=train)\n",
    "print(\"\\nTest Columns\")\n",
    "test = test_cleaner.convert_float_to_int(\"ADAS11\", method=\"round\", dataset=test)\n",
    "test = test_cleaner.convert_float_to_int(\"ADAS13\", method=\"round\", dataset=test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformation** is another step of Preprocessing. In Data Transformation, the data are transformed or consolidated into forms appropriate for mining. Strategies for data transformation include Normalization and Discretization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer = ADNITransformator(train)\n",
    "test_transformer = ADNITransformator(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADNITransformer** is a utility class for preprocessing the ***ADNIMERGE*** dataset. It stores a DataFrame and provides utilities to create ratio columns, normalize numeric features (Min-Max, Z-score, Robust), encode categorical variables (label, one-hot, ordinal), and perform different types of binning (equal-width, equal-frequency, custom). Each method updates and returns the internal dataset and prints a short summary of the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency of TAU/ABETA and PTAU/ABETA ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This study [Predicting clinical decline and\n",
    "conversion to Alzheimer's disease\n",
    "or dementia using novel Elecsys\n",
    "Aβ(1–42), pTau, and tTau CSF\n",
    "immunoassays](https://www.nature.com/articles/s41598-019-54204-z) demonstrated that the TAU/ABETA and PTAU/ABETA ratios are more predictive than the individual TAU and PTAU. So we can calculate and add the ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Columns\n",
      "Ratio column 'TAU/ABETA' created from 'TAU' / 'ABETA'.\n",
      "NaNs -> TAU: 0, ABETA: 0, TAU/ABETA: 0\n",
      "'TAU/ABETA' has 0 more NaN(s) than 'TAU' and 0 more NaN(s) than 'ABETA'.\n",
      "Ratio column 'PTAU/ABETA' created from 'PTAU' / 'ABETA'.\n",
      "NaNs -> PTAU: 0, ABETA: 0, PTAU/ABETA: 0\n",
      "'PTAU/ABETA' has 0 more NaN(s) than 'PTAU' and 0 more NaN(s) than 'ABETA'.\n",
      "\n",
      "Test Columns\n",
      "Ratio column 'TAU/ABETA' created from 'TAU' / 'ABETA'.\n",
      "NaNs -> TAU: 0, ABETA: 0, TAU/ABETA: 0\n",
      "'TAU/ABETA' has 0 more NaN(s) than 'TAU' and 0 more NaN(s) than 'ABETA'.\n",
      "Ratio column 'PTAU/ABETA' created from 'PTAU' / 'ABETA'.\n",
      "NaNs -> PTAU: 0, ABETA: 0, PTAU/ABETA: 0\n",
      "'PTAU/ABETA' has 0 more NaN(s) than 'PTAU' and 0 more NaN(s) than 'ABETA'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Columns\")\n",
    "train = train_transformer.create_ratio_column(\"TAU\", \"ABETA\", new_col_name=\"TAU/ABETA\")\n",
    "train = train_transformer.create_ratio_column(\"PTAU\", \"ABETA\", new_col_name=\"PTAU/ABETA\")\n",
    "print(\"\\nTest Columns\")\n",
    "test = test_transformer.create_ratio_column(\"TAU\", \"ABETA\", new_col_name=\"TAU/ABETA\")\n",
    "test = test_transformer.create_ratio_column(\"PTAU\", \"ABETA\", new_col_name=\"PTAU/ABETA\")\n",
    "\n",
    "train.drop([\"TAU\", \"PTAU\", \"ABETA\"], axis=1, inplace=True)\n",
    "test.drop([\"TAU\", \"PTAU\", \"ABETA\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICV Normalization for MRI Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ICV` (Intracranial Volume) reflects the overall size of the skull, which depends on gender, ethnicity, height, and genetics. People with larger skulls have larger absolute brain volumes, even without pathology, and therefore, without correction, we risk confusing \"large skull\" with \"healthy brain\". It also removes the differences in size between male and female MRIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Columns\n",
      "Ratio column 'Ventricles/ICV' created from 'Ventricles' / 'ICV'.\n",
      "NaNs -> Ventricles: 0, ICV: 0, Ventricles/ICV: 0\n",
      "'Ventricles/ICV' has 0 more NaN(s) than 'Ventricles' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Hippocampus/ICV' created from 'Hippocampus' / 'ICV'.\n",
      "NaNs -> Hippocampus: 0, ICV: 0, Hippocampus/ICV: 0\n",
      "'Hippocampus/ICV' has 0 more NaN(s) than 'Hippocampus' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Entorhinal/ICV' created from 'Entorhinal' / 'ICV'.\n",
      "NaNs -> Entorhinal: 0, ICV: 0, Entorhinal/ICV: 0\n",
      "'Entorhinal/ICV' has 0 more NaN(s) than 'Entorhinal' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Fusiform/ICV' created from 'Fusiform' / 'ICV'.\n",
      "NaNs -> Fusiform: 0, ICV: 0, Fusiform/ICV: 0\n",
      "'Fusiform/ICV' has 0 more NaN(s) than 'Fusiform' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'MidTemp/ICV' created from 'MidTemp' / 'ICV'.\n",
      "NaNs -> MidTemp: 0, ICV: 0, MidTemp/ICV: 0\n",
      "'MidTemp/ICV' has 0 more NaN(s) than 'MidTemp' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'WholeBrain/ICV' created from 'WholeBrain' / 'ICV'.\n",
      "NaNs -> WholeBrain: 0, ICV: 0, WholeBrain/ICV: 0\n",
      "'WholeBrain/ICV' has 0 more NaN(s) than 'WholeBrain' and 0 more NaN(s) than 'ICV'.\n",
      "\n",
      "Test Columns\n",
      "Ratio column 'Ventricles/ICV' created from 'Ventricles' / 'ICV'.\n",
      "NaNs -> Ventricles: 0, ICV: 0, Ventricles/ICV: 0\n",
      "'Ventricles/ICV' has 0 more NaN(s) than 'Ventricles' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Hippocampus/ICV' created from 'Hippocampus' / 'ICV'.\n",
      "NaNs -> Hippocampus: 0, ICV: 0, Hippocampus/ICV: 0\n",
      "'Hippocampus/ICV' has 0 more NaN(s) than 'Hippocampus' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Entorhinal/ICV' created from 'Entorhinal' / 'ICV'.\n",
      "NaNs -> Entorhinal: 0, ICV: 0, Entorhinal/ICV: 0\n",
      "'Entorhinal/ICV' has 0 more NaN(s) than 'Entorhinal' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'Fusiform/ICV' created from 'Fusiform' / 'ICV'.\n",
      "NaNs -> Fusiform: 0, ICV: 0, Fusiform/ICV: 0\n",
      "'Fusiform/ICV' has 0 more NaN(s) than 'Fusiform' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'MidTemp/ICV' created from 'MidTemp' / 'ICV'.\n",
      "NaNs -> MidTemp: 0, ICV: 0, MidTemp/ICV: 0\n",
      "'MidTemp/ICV' has 0 more NaN(s) than 'MidTemp' and 0 more NaN(s) than 'ICV'.\n",
      "Ratio column 'WholeBrain/ICV' created from 'WholeBrain' / 'ICV'.\n",
      "NaNs -> WholeBrain: 0, ICV: 0, WholeBrain/ICV: 0\n",
      "'WholeBrain/ICV' has 0 more NaN(s) than 'WholeBrain' and 0 more NaN(s) than 'ICV'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Columns\")\n",
    "train = train_transformer.create_ratio_column(\"Ventricles\", \"ICV\", new_col_name=\"Ventricles/ICV\")\n",
    "train = train_transformer.create_ratio_column(\"Hippocampus\", \"ICV\", new_col_name=\"Hippocampus/ICV\")\n",
    "train = train_transformer.create_ratio_column(\"Entorhinal\", \"ICV\", new_col_name=\"Entorhinal/ICV\")\n",
    "train = train_transformer.create_ratio_column(\"Fusiform\", \"ICV\", new_col_name=\"Fusiform/ICV\")\n",
    "train = train_transformer.create_ratio_column(\"MidTemp\", \"ICV\", new_col_name=\"MidTemp/ICV\")\n",
    "train = train_transformer.create_ratio_column(\"WholeBrain\", \"ICV\", new_col_name=\"WholeBrain/ICV\")\n",
    "print(\"\\nTest Columns\")\n",
    "test = test_transformer.create_ratio_column(\"Ventricles\", \"ICV\", new_col_name=\"Ventricles/ICV\")\n",
    "test = test_transformer.create_ratio_column(\"Hippocampus\", \"ICV\", new_col_name=\"Hippocampus/ICV\")\n",
    "test = test_transformer.create_ratio_column(\"Entorhinal\", \"ICV\", new_col_name=\"Entorhinal/ICV\")\n",
    "test = test_transformer.create_ratio_column(\"Fusiform\", \"ICV\", new_col_name=\"Fusiform/ICV\")\n",
    "test = test_transformer.create_ratio_column(\"MidTemp\", \"ICV\", new_col_name=\"MidTemp/ICV\")\n",
    "test = test_transformer.create_ratio_column(\"WholeBrain\", \"ICV\", new_col_name=\"WholeBrain/ICV\")\n",
    "\n",
    "train.drop([\"Ventricles\", \"Hippocampus\", \"Entorhinal\", \"Fusiform\", \"MidTemp\", \"WholeBrain\", \"ICV\"], axis=1, inplace=True)\n",
    "test.drop([\"Ventricles\", \"Hippocampus\", \"Entorhinal\", \"Fusiform\", \"MidTemp\", \"WholeBrain\", \"ICV\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data reduction is a preprocessing step in data analysis aimed at minimizing the volume of data while preserving its essential information and structure. It helps improve efficiency, reduce storage needs, and speed up processing by using techniques such as feature selection, dimensionality reduction, and data aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of choosing the most relevant variables or attributes from a dataset to improve model performance, reduce complexity, and prevent overfitting.\n",
    "\n",
    "In this part, we will remove columns that are unnecessary for the classification of Alzheimer's disease diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alzheimer's Disease Assessment Scale (ADAS) Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ADAS11` and `ADAS13` are two versions of the same rating scale used to measure the severity of cognitive symptoms in Alzheimer's disease.\n",
    "\n",
    "`ADAS11` is the original version with 11 items assessing memory, language, attention, and other cognitive functions.\n",
    "\n",
    "`ADAS13` is an extended version that adds two additional items (making it 13 in total) to improve the sensitivity of the assessment, especially in the early stages of the disease.\n",
    "\n",
    "Since this is an upgrade, we could keep only `ADAS13`, but first we want to see the correlation index. If it's very high, we can remove `ADAS11` since it's very redundant with `ADAS13`.\n",
    "\n",
    "Regarding `ADASQ4` (ADAS Delayed Word Recall), it represents a subset of the ADAS assessment focused primarily on memory recall ability, specifically the delayed word recall task. It is important to note that `ADAS13` includes the **Delayed Word Recall** item along with **Number Cancellation**, which are not part of `ADAS11`. This means that `ADASQ4` captures a more specific cognitive domain related to memory function, which can be particularly sensitive to early cognitive decline. Therefore, while `ADASQ4` does not provide a full cognitive profile like `ADAS11` or `ADAS13`, it serves as a valuable measure for detecting subtle memory impairments linked to Alzheimer's disease progression. Let's make a comparison similar to the one before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between ADAS11 and ADAS13: \t 0.9751383118275905\n",
      "Correlation between ADASQ4 and ADAS13: \t 0.8749180693418157\n"
     ]
    }
   ],
   "source": [
    "correlation = train['ADAS11'].corr(train['ADAS13'])\n",
    "print(f\"Correlation between ADAS11 and ADAS13: \\t {correlation}\")\n",
    "\n",
    "correlation = train['ADASQ4'].corr(train['ADAS13'])\n",
    "print(f\"Correlation between ADASQ4 and ADAS13: \\t {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high correlations, `ADASQ4` and `ADAS11` appear largely redundant when `ADAS13` is already included. Since `ADAS13` is a more comprehensive scale that encompasses `ADAS11` plus additional items including delayed word recall (which corresponds to `ADASQ4`) and number cancellation, it is reasonable to remove both from further analysis. \n",
    "\n",
    "However, **correlation** only measures the **information redundancy** in a linear way. We want to verify whether `ADAS11` and `ADASQ4` add useful information compared to `ADAS13`, or whether they are redundant because `ADAS13` can be reconstructed almost entirely from them. Since `ADAS13` includes the subscores of `ADAS11` and `ADASQ4`, the hypothesis is that the linear combination of the two explains almost all of the variability of `ADAS13`.\n",
    "\n",
    "A **multivariate linear regressio**n is applied with `ADAS13` as the dependent variable and `ADAS11` and `ADASQ4` as predictors. The model estimates the coefficients (β_0, β_1, β_2) by minimizing the residual error between actual and predicted values, thus measuring how well the two subscores reconstruct `ADAS13`.\n",
    "\n",
    "If the model shows an **R² ≥ 0.98** (coefficient of determination) and **RMSE < 1** (RMSE = Root Mean Squared Error), then `ADAS13` is almost perfectly explained by `ADAS11` and `ADASQ4`. In this case, the latter can be removed to avoid redundancy and collinearity.\n",
    "\n",
    "**This criterion is chosen because it provides an objective and interpretable measure of information overlap, more robust than isolated correlations or normalizations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate linear model\n",
      "R2: 0.9896790043891479\n",
      "Coefficients (ADAS11, ADASQ4): [1.08187574 0.99232526] Intercept: 0.03308208620889275\n",
      "Root Mean Squared Error: 0.9381672638012968 \n",
      "Max Error: 9.897404939322776\n"
     ]
    }
   ],
   "source": [
    "X = train[['ADAS11','ADASQ4']].astype(float)\n",
    "y = train['ADAS13'].astype(float)\n",
    "\n",
    "# Multivariate Linear Regression\n",
    "lr = LinearRegression().fit(X, y)\n",
    "pred = lr.predict(X)\n",
    "r2 = lr.score(X, y)\n",
    "rmse = mean_squared_error(y, pred)\n",
    "max_err = np.abs(y - pred).max()\n",
    "pct_within_1 = (np.abs(y - pred) <= 1.0).mean() * 100\n",
    "\n",
    "print(\"Multivariate linear model\")\n",
    "print(\"R2:\", r2)\n",
    "print(\"Coefficients (ADAS11, ADASQ4):\", lr.coef_, \"Intercept:\", lr.intercept_)\n",
    "print(\"Root Mean Squared Error:\", rmse, \"\\nMax Error:\", max_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **linear model** explains almost all of the variance in `ADAS13` (**R² = 0.99**) using `ADAS11` and `ADASQ4`, with coefficients very close to 1.\n",
    "\n",
    "The mean error (**RMSE = 0.94**) is very small compared to the 0–85 scale, so the estimate is clinically accurate. \n",
    "\n",
    "The fact that 78.8% of cases show a difference between the observed and predicted value less than or equal to ±1 point indicates that, for most subjects, the model reconstructs ADAS13 with minimal and clinically negligible error. However, approximately one-fifth of the cases show a deviation above this threshold, indicating a slight residual dispersion: the model is not perfectly accurate for all subjects, but the error remains small and compatible with the expected variability in cognitive scores.\n",
    "\n",
    "We drop the two attributes in favor of `ADAS13`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"ADAS11\", \"ADASQ4\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everyday Cognition (Ecog) Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if `EcogPtTotal`, `EcogSPTotal` can be substituted by their component attributes:\n",
    "- `EcogPtMem`, `EcogPtLang`, `EcogPtVisspat`, `EcogPtPlan`, `EcogPtOrgan`, `EcogPtDivatt` for `EcogPtTotal`.\n",
    "- `EcogSPMem`, `EcogSPLang`, `EcogSPVisspat`, `EcogSPPlan`, `EcogSPOrgan`, `EcogSPDivatt` for `EcogSPTotal`.\n",
    "\n",
    "The **Total** should be the overall average of all these attributes and we check if it is true or at least very close to it. By comparing the totals to the sum and mean of the components, and computing Pearson correlations, we evaluate how closely the totals reflect aggregated component values. Additionally, correlations between individual components and totals highlight which attributes influence the totals most. \n",
    "\n",
    "The results guide whether the total scores provide distinct information or can be omitted in favor of the component measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EcogPtTotal Analysis ===\n",
      "Max absolute difference between EcogPtTotal and sum of EcogPt components: 18.153220\n",
      "Max absolute difference between EcogPtTotal and mean of EcogPt components: 0.370070\n",
      "\n",
      "=== EcogSPTotal Analysis ===\n",
      "Max absolute difference between EcogSPTotal and sum of EcogSP components: 19.773500\n",
      "Max absolute difference between EcogSPTotal and mean of EcogSP components: 0.786707\n",
      "\n",
      "=== Correlation with sum ===\n",
      "Correlation between EcogPtTotal and sum of EcogPt components: 0.9960\n",
      "Correlation between EcogSPTotal and sum of EcogSP components: 0.9972\n",
      "\n",
      "=== Correlation with mean ===\n",
      "Correlation between EcogPtTotal and mean of EcogPt components: 0.9960\n",
      "Correlation between EcogSPTotal and mean of EcogSP components: 0.9972\n",
      "\n",
      "Correlations between EcogPtTotal and EcogPt attributes:\n",
      "EcogPtMem: 0.8613\n",
      "EcogPtLang: 0.8929\n",
      "EcogPtVisspat: 0.8468\n",
      "EcogPtPlan: 0.8552\n",
      "EcogPtOrgan: 0.8256\n",
      "EcogPtDivatt: 0.8228\n",
      "\n",
      "Correlations between EcogSPTotal and EcogSP attributes:\n",
      "EcogSPMem: 0.9236\n",
      "EcogSPLang: 0.9145\n",
      "EcogSPVisspat: 0.8995\n",
      "EcogSPPlan: 0.9324\n",
      "EcogSPOrgan: 0.9081\n",
      "EcogSPDivatt: 0.9071\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Columns related to EcogPt and EcogSP scores (excluding the Total columns)\n",
    "ecogpt_cols = ['EcogPtMem', 'EcogPtLang', 'EcogPtVisspat', 'EcogPtPlan', 'EcogPtOrgan', 'EcogPtDivatt']\n",
    "ecogsp_cols = ['EcogSPMem', 'EcogSPLang', 'EcogSPVisspat', 'EcogSPPlan', 'EcogSPOrgan', 'EcogSPDivatt']\n",
    "\n",
    "# Calculate sum and mean for EcogPt attributes\n",
    "ecogpt_sum = train[ecogpt_cols].sum(axis=1)\n",
    "ecogpt_mean = train[ecogpt_cols].mean(axis=1)\n",
    "\n",
    "# Calculate sum and mean for EcogSP attributes\n",
    "ecogsp_sum = train[ecogsp_cols].sum(axis=1)\n",
    "ecogsp_mean = train[ecogsp_cols].mean(axis=1)\n",
    "\n",
    "# Calculate absolute differences between Total and sum/mean for EcogPtTotal\n",
    "diff_pt_sum = (ecogpt_sum - train['EcogPtTotal']).abs()\n",
    "diff_pt_mean = (ecogpt_mean - train['EcogPtTotal']).abs()\n",
    "\n",
    "# Calculate absolute differences between Total and sum/mean for EcogSPTotal\n",
    "diff_sp_sum = (ecogsp_sum - train['EcogSPTotal']).abs()\n",
    "diff_sp_mean = (ecogsp_mean - train['EcogSPTotal']).abs()\n",
    "\n",
    "# Print max differences to check if Total columns correspond to sum or mean\n",
    "print(\"=== EcogPtTotal Analysis ===\")\n",
    "print(f\"Max absolute difference between EcogPtTotal and sum of EcogPt components: {diff_pt_sum.max():.6f}\")\n",
    "print(f\"Max absolute difference between EcogPtTotal and mean of EcogPt components: {diff_pt_mean.max():.6f}\")\n",
    "\n",
    "print(\"\\n=== EcogSPTotal Analysis ===\")\n",
    "print(f\"Max absolute difference between EcogSPTotal and sum of EcogSP components: {diff_sp_sum.max():.6f}\")\n",
    "print(f\"Max absolute difference between EcogSPTotal and mean of EcogSP components: {diff_sp_mean.max():.6f}\")\n",
    "\n",
    "# Compute Pearson correlation coefficients between Total and sum for both EcogPt and EcogSP\n",
    "corr_pt_sum = train['EcogPtTotal'].corr(ecogpt_sum)\n",
    "corr_sp_sum = train['EcogSPTotal'].corr(ecogsp_sum)\n",
    "\n",
    "print(\"\\n=== Correlation with sum ===\")\n",
    "print(f\"Correlation between EcogPtTotal and sum of EcogPt components: {corr_pt_sum:.4f}\")\n",
    "print(f\"Correlation between EcogSPTotal and sum of EcogSP components: {corr_sp_sum:.4f}\")\n",
    "\n",
    "# Compute Pearson correlation coefficients between Total and mean for both EcogPt and EcogSP\n",
    "corr_pt_mean = train['EcogPtTotal'].corr(ecogpt_mean)\n",
    "corr_sp_mean = train['EcogSPTotal'].corr(ecogsp_mean)\n",
    "\n",
    "print(\"\\n=== Correlation with mean ===\")\n",
    "print(f\"Correlation between EcogPtTotal and mean of EcogPt components: {corr_pt_mean:.4f}\")\n",
    "print(f\"Correlation between EcogSPTotal and mean of EcogSP components: {corr_sp_mean:.4f}\")\n",
    "\n",
    "# Calculate Pearson correlations between EcogPtTotal and individual EcogPt attributes\n",
    "corrs_pt = {}\n",
    "for col in ecogpt_cols:\n",
    "    corr = train['EcogPtTotal'].corr(train[col])\n",
    "    corrs_pt[col] = corr\n",
    "\n",
    "# Calculate Pearson correlations between EcogSPTotal and individual EcogSP attributes\n",
    "corrs_sp = {}\n",
    "for col in ecogsp_cols:\n",
    "    corr = train['EcogSPTotal'].corr(train[col])\n",
    "    corrs_sp[col] = corr\n",
    "\n",
    "# Print correlations before plotting\n",
    "print(\"\\nCorrelations between EcogPtTotal and EcogPt attributes:\")\n",
    "for attr, corr_val in corrs_pt.items():\n",
    "    print(f\"{attr}: {corr_val:.4f}\")\n",
    "\n",
    "print(\"\\nCorrelations between EcogSPTotal and EcogSP attributes:\")\n",
    "for attr, corr_val in corrs_sp.items():\n",
    "    print(f\"{attr}: {corr_val:.4f}\")\n",
    "\n",
    "# Plotting correlations as bar charts\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart for EcogPtTotal correlations\n",
    "axes[0].bar(corrs_pt.keys(), corrs_pt.values(), color='skyblue')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_title('Correlation between EcogPtTotal and EcogPt attributes')\n",
    "axes[0].set_ylabel('Pearson correlation')\n",
    "axes[0].set_xticks(range(len(corrs_pt)))\n",
    "axes[0].set_xticklabels(corrs_pt.keys(), rotation=45, ha='right')\n",
    "\n",
    "# Bar chart for EcogSPTotal correlations\n",
    "axes[1].bar(corrs_sp.keys(), corrs_sp.values(), color='lightgreen')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('Correlation between EcogSPTotal and EcogSP attributes')\n",
    "axes[1].set_ylabel('Pearson correlation')\n",
    "axes[1].set_xticks(range(len(corrs_sp)))\n",
    "axes[1].set_xticklabels(corrs_sp.keys(), rotation=45, ha='right')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`EcogPtTotal` and `EcogSPTotal` are effectively redundant with their component scores: the totals correlate ≈ 0.996–0.998 with the component mean, while individual components correlate strongly with their respective totals (PT: 0.82–0.90; SP: 0.89–0.94), with SP components slightly tighter. We drop the Total columns and retain the six component variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"EcogPtTotal\", \"EcogSPTotal\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composite Scores (mPACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mPACCdigit` and `mPACCtrailsB` are two modified versions of the Preclinical Alzheimer’s Cognitive Composite (PACC), calculated within the ADNI dataset. In the original version, the PACC combines tests such as FCSRT (*Free and Cued Selective Reminding Test*), Logical Memory II, Digit Symbol Substitution and MMSE, but ADNI does not include FCSRT (which is why it is modified).\n",
    "- `mPACCdigit` uses the Digit Symbol Substitution Test (`DIGITSCOR`) instead of the FCSRT. It is more sensitive to processing speed and attention.\n",
    "- `mPACCtrailsB` uses the Trail Making Test Part B (`TRABSCOR`) instead of the FCSRT. It is more sensitive to executive functions and multitasking.\n",
    "\n",
    "Therefore they are higly similar. We measure the correlation between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between mPACCdigit and mPACCtrailsB: \t 0.978570266302394\n"
     ]
    }
   ],
   "source": [
    "correlation = train[\"mPACCdigit\"].corr(train[\"mPACCtrailsB\"])\n",
    "print(f\"Correlation between mPACCdigit and mPACCtrailsB: \\t {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very high and therefore the information is practically redundant. We see which one has the bigger mutual information in relation to the **Diagnosis**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mPACCdigit      0.646655\n",
      "mPACCtrailsB    0.593376\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = train[[\"mPACCdigit\", \"mPACCtrailsB\"]]\n",
    "y = train['DX']  # target \n",
    "\n",
    "mi = mutual_info_classif(X, y, random_state=42)\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "print(mi_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although they are very close to each other, we prefer to keep `mPACCdigit`. Furthermore we can say that `DIGITSCOR` was removed during data cleaning due to the high percentage of NULL values, while `TRABSCOR` is still present. Therefore, we decided to drop `mPACCtrailsB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"mPACCtrailsB\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `MARRIED`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how many people are married and how many are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARRIED\n",
      "1    1451\n",
      "0     483\n",
      "Name: count, dtype: int64\n",
      "MARRIED\n",
      "1    75.03%\n",
      "0    24.97%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train[\"MARRIED\"].value_counts())\n",
    "print(((train[\"MARRIED\"].value_counts(normalize=True) * 100).round(2).astype(str) + '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75% of people are *married*. This attribute is too unbalanced and we assume it isn't very predictive of diagnosis. So we're removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"MARRIED\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `PTDEMOGROUP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see the distribution of demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTDEMOGROUP\n",
      "6    1633\n",
      "2     149\n",
      "4      79\n",
      "1      43\n",
      "5      23\n",
      "0       5\n",
      "3       2\n",
      "Name: count, dtype: int64\n",
      "PTDEMOGROUP\n",
      "6    84.44%\n",
      "2      7.7%\n",
      "4     4.08%\n",
      "1     2.22%\n",
      "5     1.19%\n",
      "0     0.26%\n",
      "3      0.1%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train[\"PTDEMOGROUP\"].value_counts())\n",
    "print(((train[\"PTDEMOGROUP\"].value_counts(normalize=True) * 100).round(2).astype(str) + '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "84% of people are *white*. This attribute is too unbalanced. So we're removing it. It's a shame, because if it had been more heterogeneous we could have seen some interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"PTDEMOGROUP\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset with Hybrid Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have observed that the classes are slightly **unbalanced**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution (count):\n",
      "DX\n",
      "CN      717\n",
      "LMCI    548\n",
      "EMCI    336\n",
      "AD      333\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original class distribution (percentages):\n",
      "DX\n",
      "CN      37.07%\n",
      "LMCI    28.34%\n",
      "EMCI    17.37%\n",
      "AD      17.22%\n",
      "Name: proportion, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Original class distribution (count):\")\n",
    "print(train['DX'].value_counts())\n",
    "print(\"\\nOriginal class distribution (percentages):\")\n",
    "print(((train['DX'].value_counts(normalize=True) * 100).round(2).astype(str) + '%'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are unbalanced classes, models tend to ignore them (**majority bias**), leading to **low recall** for the minority. To solve this problem we use the **Hybrid Sampling**.\n",
    "\n",
    "Hybrid Sampling consists of two operations:\n",
    "- **Random UnderSampling (RUS):** Reduces the majority class by eliminating random observations;\n",
    "- **Synthetic Minority Over-sampling Technique for Nominal and Continuous (SMOTENC):** Generates synthetic samples for the minority by interpolating real examples (also handles categorical features).\n",
    "\n",
    "Hybrid Sampling is useful when you want to improve the representation of minority groups without destroying the information structure of the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Undersample dict (RUS) -> classes to reduce:\n",
      "{'CN': 500, 'LMCI': 500}\n",
      "\n",
      "Oversample dict (SMOTENC) -> classes to increase:\n",
      "{'EMCI': 500, 'AD': 500}\n"
     ]
    }
   ],
   "source": [
    "# Undersampling strategy: only for classes larger than target_count\n",
    "undersample_dict = {\"CN\": 500, \"LMCI\": 500}\n",
    "\n",
    "# Oversampling strategy: only for classes smaller than target_count\n",
    "oversample_dict = {\"EMCI\": 500, \"AD\": 500}\n",
    "\n",
    "categorical_features = [\n",
    "    train.columns.get_loc(\"PTGENDER\"),\n",
    "    train.columns.get_loc(\"APOE4\")\n",
    "]\n",
    "\n",
    "print(\"\\nUndersample dict (RUS) -> classes to reduce:\")\n",
    "print(undersample_dict)\n",
    "print(\"\\nOversample dict (SMOTENC) -> classes to increase:\")\n",
    "print(oversample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution after resampling (count):\n",
      "DX\n",
      "CN      500\n",
      "EMCI    500\n",
      "LMCI    500\n",
      "AD      500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution after resampling (percentages):\n",
      "DX\n",
      "CN      25.0%\n",
      "EMCI    25.0%\n",
      "LMCI    25.0%\n",
      "AD      25.0%\n",
      "Name: proportion, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>72.544500</td>\n",
       "      <td>7.245914</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTGENDER</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.501500</td>\n",
       "      <td>0.500123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>16.002000</td>\n",
       "      <td>2.780443</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APOE4</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.643239</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDRSB</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.819250</td>\n",
       "      <td>1.898751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAS13</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>17.426000</td>\n",
       "      <td>9.900128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMSE</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>26.870500</td>\n",
       "      <td>2.859689</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>34.598000</td>\n",
       "      <td>12.497299</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.118500</td>\n",
       "      <td>2.753225</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_forgetting</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.336500</td>\n",
       "      <td>2.431918</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>60.169381</td>\n",
       "      <td>34.664104</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>30.769200</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>6.876000</td>\n",
       "      <td>5.224871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRABSCOR</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>127.032500</td>\n",
       "      <td>78.843951</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAQ</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>4.746000</td>\n",
       "      <td>6.571002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mPACCdigit</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>-6.559925</td>\n",
       "      <td>6.416531</td>\n",
       "      <td>-23.603400</td>\n",
       "      <td>-11.480600</td>\n",
       "      <td>-5.623185</td>\n",
       "      <td>-1.415796</td>\n",
       "      <td>6.300310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOCA</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>22.251000</td>\n",
       "      <td>4.293975</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtMem</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.168502</td>\n",
       "      <td>0.641526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.584054</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtLang</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.785733</td>\n",
       "      <td>0.575242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>2.111110</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.450262</td>\n",
       "      <td>0.484999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.099840</td>\n",
       "      <td>1.309525</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>3.833330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.460918</td>\n",
       "      <td>0.503387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.562353</td>\n",
       "      <td>0.550990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.453333</td>\n",
       "      <td>1.833330</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.856419</td>\n",
       "      <td>0.644585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.409129</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.200588</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPMem</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.243351</td>\n",
       "      <td>0.887666</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.475000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPLang</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.753915</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.555535</td>\n",
       "      <td>2.222934</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.602200</td>\n",
       "      <td>0.712399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.306042</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.700207</td>\n",
       "      <td>0.770229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.148178</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.806525</td>\n",
       "      <td>0.837844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.019346</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>2.009636</td>\n",
       "      <td>0.873217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.696984</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FDG</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>1.193807</td>\n",
       "      <td>0.150715</td>\n",
       "      <td>0.621933</td>\n",
       "      <td>1.089232</td>\n",
       "      <td>1.206360</td>\n",
       "      <td>1.304920</td>\n",
       "      <td>1.735930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAU/ABETA</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.390363</td>\n",
       "      <td>0.263611</td>\n",
       "      <td>0.073807</td>\n",
       "      <td>0.182951</td>\n",
       "      <td>0.315874</td>\n",
       "      <td>0.546292</td>\n",
       "      <td>2.400721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.038499</td>\n",
       "      <td>0.028202</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>0.016539</td>\n",
       "      <td>0.029608</td>\n",
       "      <td>0.054906</td>\n",
       "      <td>0.245297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>0.012838</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.033695</td>\n",
       "      <td>0.078630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.005110</td>\n",
       "      <td>0.007062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.005052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.006960</td>\n",
       "      <td>0.010329</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.017320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.011597</td>\n",
       "      <td>0.012931</td>\n",
       "      <td>0.014205</td>\n",
       "      <td>0.019746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>0.678010</td>\n",
       "      <td>0.053049</td>\n",
       "      <td>0.511875</td>\n",
       "      <td>0.638876</td>\n",
       "      <td>0.681007</td>\n",
       "      <td>0.716931</td>\n",
       "      <td>0.831140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count        mean        std         min        25%  \\\n",
       "AGE                    2000.0   72.544500   7.245914   53.000000  68.000000   \n",
       "PTGENDER               2000.0    0.501500   0.500123    0.000000   0.000000   \n",
       "PTEDUCAT               2000.0   16.002000   2.780443    4.000000  14.000000   \n",
       "APOE4                  2000.0    0.549500   0.643239    0.000000   0.000000   \n",
       "CDRSB                  2000.0    1.819250   1.898751    0.000000   0.500000   \n",
       "ADAS13                 2000.0   17.426000   9.900128    0.000000  10.000000   \n",
       "MMSE                   2000.0   26.870500   2.859689   16.000000  25.000000   \n",
       "RAVLT_immediate        2000.0   34.598000  12.497299    1.000000  25.000000   \n",
       "RAVLT_learning         2000.0    4.118500   2.753225   -4.000000   2.000000   \n",
       "RAVLT_forgetting       2000.0    4.336500   2.431918   -5.000000   3.000000   \n",
       "RAVLT_perc_forgetting  2000.0   60.169381  34.664104 -100.000000  30.769200   \n",
       "LDELTOTAL              2000.0    6.876000   5.224871    0.000000   2.000000   \n",
       "TRABSCOR               2000.0  127.032500  78.843951   21.000000  70.000000   \n",
       "FAQ                    2000.0    4.746000   6.571002    0.000000   0.000000   \n",
       "mPACCdigit             2000.0   -6.559925   6.416531  -23.603400 -11.480600   \n",
       "MOCA                   2000.0   22.251000   4.293975    4.000000  20.000000   \n",
       "EcogPtMem              2000.0    2.168502   0.641526    1.000000   1.725000   \n",
       "EcogPtLang             2000.0    1.785733   0.575242    1.000000   1.333330   \n",
       "EcogPtVisspat          2000.0    1.450262   0.484999    1.000000   1.099840   \n",
       "EcogPtPlan             2000.0    1.460918   0.503387    1.000000   1.000000   \n",
       "EcogPtOrgan            2000.0    1.562353   0.550990    1.000000   1.166670   \n",
       "EcogPtDivatt           2000.0    1.856419   0.644585    1.000000   1.409129   \n",
       "EcogSPMem              2000.0    2.243351   0.887666    1.000000   1.475000   \n",
       "EcogSPLang             2000.0    1.753915   0.721747    1.000000   1.111110   \n",
       "EcogSPVisspat          2000.0    1.602200   0.712399    1.000000   1.000000   \n",
       "EcogSPPlan             2000.0    1.700207   0.770229    1.000000   1.000000   \n",
       "EcogSPOrgan            2000.0    1.806525   0.837844    1.000000   1.019346   \n",
       "EcogSPDivatt           2000.0    2.009636   0.873217    1.000000   1.250000   \n",
       "FDG                    2000.0    1.193807   0.150715    0.621933   1.089232   \n",
       "TAU/ABETA              2000.0    0.390363   0.263611    0.073807   0.182951   \n",
       "PTAU/ABETA             2000.0    0.038499   0.028202    0.006020   0.016539   \n",
       "Ventricles/ICV         2000.0    0.026278   0.012838    0.004600   0.016711   \n",
       "Hippocampus/ICV        2000.0    0.004489   0.000826    0.001998   0.003842   \n",
       "Entorhinal/ICV         2000.0    0.002332   0.000548    0.000926   0.001939   \n",
       "Fusiform/ICV           2000.0    0.011539   0.001713    0.006960   0.010329   \n",
       "MidTemp/ICV            2000.0    0.012902   0.001867    0.006109   0.011597   \n",
       "WholeBrain/ICV         2000.0    0.678010   0.053049    0.511875   0.638876   \n",
       "\n",
       "                             50%         75%         max  \n",
       "AGE                    73.000000   78.000000   91.000000  \n",
       "PTGENDER                1.000000    1.000000    1.000000  \n",
       "PTEDUCAT               16.000000   18.000000   20.000000  \n",
       "APOE4                   0.000000    1.000000    2.000000  \n",
       "CDRSB                   1.000000    3.000000   10.000000  \n",
       "ADAS13                 16.000000   24.000000   55.000000  \n",
       "MMSE                   28.000000   29.000000   30.000000  \n",
       "RAVLT_immediate        33.000000   43.000000   70.000000  \n",
       "RAVLT_learning          4.000000    6.000000   12.000000  \n",
       "RAVLT_forgetting        4.000000    6.000000   14.000000  \n",
       "RAVLT_perc_forgetting  60.000000  100.000000  100.000000  \n",
       "LDELTOTAL               7.000000   10.000000   22.000000  \n",
       "TRABSCOR               97.000000  160.000000  300.000000  \n",
       "FAQ                     1.000000    8.000000   30.000000  \n",
       "mPACCdigit             -5.623185   -1.415796    6.300310  \n",
       "MOCA                   23.000000   25.000000   30.000000  \n",
       "EcogPtMem               2.125000    2.584054    4.000000  \n",
       "EcogPtLang              1.666670    2.111110    4.000000  \n",
       "EcogPtVisspat           1.309525    1.666670    3.833330  \n",
       "EcogPtPlan              1.360000    1.680000    4.000000  \n",
       "EcogPtOrgan             1.453333    1.833330    4.000000  \n",
       "EcogPtDivatt            1.750000    2.200588    4.000000  \n",
       "EcogSPMem               2.125000    3.000000    4.000000  \n",
       "EcogSPLang              1.555535    2.222934    4.000000  \n",
       "EcogSPVisspat           1.306042    2.000000    4.000000  \n",
       "EcogSPPlan              1.400000    2.148178    4.000000  \n",
       "EcogSPOrgan             1.500000    2.333330    4.000000  \n",
       "EcogSPDivatt            1.800000    2.696984    4.000000  \n",
       "FDG                     1.206360    1.304920    1.735930  \n",
       "TAU/ABETA               0.315874    0.546292    2.400721  \n",
       "PTAU/ABETA              0.029608    0.054906    0.245297  \n",
       "Ventricles/ICV          0.023484    0.033695    0.078630  \n",
       "Hippocampus/ICV         0.004499    0.005110    0.007062  \n",
       "Entorhinal/ICV          0.002343    0.002704    0.005052  \n",
       "Fusiform/ICV            0.011529    0.012794    0.017320  \n",
       "MidTemp/ICV             0.012931    0.014205    0.019746  \n",
       "WholeBrain/ICV          0.681007    0.716931    0.831140  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = []\n",
    "\n",
    "rus = RandomUnderSampler(sampling_strategy=undersample_dict, random_state=42)\n",
    "steps.append((\"rus\", rus))\n",
    "\n",
    "smotenc = SMOTENC(categorical_features=categorical_features, sampling_strategy=oversample_dict, random_state=42)\n",
    "steps.append((\"smotenc\", smotenc))\n",
    "\n",
    "X_train = train.drop(columns=['DX'])\n",
    "y_train = train['DX']\n",
    "\n",
    "pipeline = Pipeline(steps=steps)\n",
    "X_res, y_res = pipeline.fit_resample(X_train, y_train)\n",
    "sampled = pd.concat([pd.DataFrame(X_res, columns=X_train.columns),\n",
    "                     pd.DataFrame(y_res, columns=['DX'])],\n",
    "                    axis=1)\n",
    "# Shuffling\n",
    "sampled = sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Distribution after resampling\n",
    "order = [\"CN\", \"EMCI\", \"LMCI\", \"AD\"]\n",
    "print(\"\\nClass distribution after resampling (count):\")\n",
    "print(sampled['DX'].value_counts().reindex(order))\n",
    "print(\"\\nClass distribution after resampling (percentages):\")\n",
    "print(((sampled['DX'].value_counts(normalize=True) * 100).astype(str) + '%').reindex(order))\n",
    "\n",
    "display(sampled.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset after Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then this is the dataset given by the Data Preprocessing procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>mPACCdigit</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>FDG</th>\n",
       "      <th>TAU/ABETA</th>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AD</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>6.5</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>300</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-20.06920</td>\n",
       "      <td>3.14286</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.75</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.444440</td>\n",
       "      <td>2.666670</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.666670</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.042620</td>\n",
       "      <td>0.630939</td>\n",
       "      <td>0.061115</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.009514</td>\n",
       "      <td>0.011188</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>0.680575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>1.5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>155</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>-10.20060</td>\n",
       "      <td>2.10000</td>\n",
       "      <td>1.780554</td>\n",
       "      <td>1.274284</td>\n",
       "      <td>1.373334</td>\n",
       "      <td>1.699998</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>2.530556</td>\n",
       "      <td>2.014286</td>\n",
       "      <td>1.756666</td>\n",
       "      <td>2.316666</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1.080580</td>\n",
       "      <td>0.421770</td>\n",
       "      <td>0.041719</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.008863</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.049299</td>\n",
       "      <td>0.640478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>106</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>-18.1818</td>\n",
       "      <td>-5.90200</td>\n",
       "      <td>2.75000</td>\n",
       "      <td>2.555560</td>\n",
       "      <td>2.285710</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>3.833330</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.510714</td>\n",
       "      <td>1.266666</td>\n",
       "      <td>1.438096</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>1.680000</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.414550</td>\n",
       "      <td>0.182691</td>\n",
       "      <td>0.016757</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.012161</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.025251</td>\n",
       "      <td>0.630110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>58</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>3.19941</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.118820</td>\n",
       "      <td>0.263255</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>0.004908</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.013293</td>\n",
       "      <td>0.044823</td>\n",
       "      <td>0.677111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>28.5714</td>\n",
       "      <td>-1.16303</td>\n",
       "      <td>1.87500</td>\n",
       "      <td>1.555560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.270141</td>\n",
       "      <td>0.197315</td>\n",
       "      <td>0.018091</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.013675</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.036393</td>\n",
       "      <td>0.726675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-10.37820</td>\n",
       "      <td>2.97500</td>\n",
       "      <td>1.930556</td>\n",
       "      <td>1.585714</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.416668</td>\n",
       "      <td>1.95</td>\n",
       "      <td>3.025000</td>\n",
       "      <td>1.955554</td>\n",
       "      <td>1.671428</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.866666</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.096238</td>\n",
       "      <td>0.709354</td>\n",
       "      <td>0.067789</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.659038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>79</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>42.8571</td>\n",
       "      <td>-9.18102</td>\n",
       "      <td>2.12500</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.833330</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.068610</td>\n",
       "      <td>0.263624</td>\n",
       "      <td>0.025132</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.030409</td>\n",
       "      <td>0.648833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>300</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>16.6667</td>\n",
       "      <td>-9.94141</td>\n",
       "      <td>1.60000</td>\n",
       "      <td>1.533332</td>\n",
       "      <td>1.219048</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>1.133334</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>1.657144</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.366666</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.130734</td>\n",
       "      <td>0.175266</td>\n",
       "      <td>0.015387</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.009460</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.019144</td>\n",
       "      <td>0.634472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-13.05080</td>\n",
       "      <td>1.87500</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>1.857140</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.116780</td>\n",
       "      <td>0.673073</td>\n",
       "      <td>0.062445</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.009382</td>\n",
       "      <td>0.040972</td>\n",
       "      <td>0.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>116</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>-9.22386</td>\n",
       "      <td>2.12500</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.714290</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.018180</td>\n",
       "      <td>0.314876</td>\n",
       "      <td>0.034044</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.011175</td>\n",
       "      <td>0.011473</td>\n",
       "      <td>0.052054</td>\n",
       "      <td>0.720471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DX  AGE  PTGENDER  PTEDUCAT  APOE4  MMSE  CDRSB  ADAS13  LDELTOTAL  \\\n",
       "0       AD   80         1        14      0    21    6.5      42          0   \n",
       "1     LMCI   82         1        20      0    24    1.5      20          2   \n",
       "2     LMCI   71         1        19      0    26    1.0       8          2   \n",
       "3       CN   75         0        20      0    30    0.0       6         19   \n",
       "4       CN   81         0        19      0    29    0.0       8         11   \n",
       "...    ...  ...       ...       ...    ...   ...    ...     ...        ...   \n",
       "1929  LMCI   64         0        14      2    27    2.0      22          0   \n",
       "1930  LMCI   82         1        18      0    28    1.5      23          5   \n",
       "1931  LMCI   76         1        12      0    25    1.0      16          3   \n",
       "1932  LMCI   74         1        19      1    26    2.0      27          3   \n",
       "1933  EMCI   72         1        14      1    26    0.5      15          7   \n",
       "\n",
       "      FAQ  MOCA  TRABSCOR  RAVLT_immediate  RAVLT_learning  \\\n",
       "0      19    13       300               15               1   \n",
       "1       4    20       155               29               0   \n",
       "2       2    25       106               51               2   \n",
       "3       0    26        58               61               7   \n",
       "4       0    29        54               54               7   \n",
       "...   ...   ...       ...              ...             ...   \n",
       "1929    9    22        62               31               1   \n",
       "1930    1    25        79               33               1   \n",
       "1931    1    22       300               27               2   \n",
       "1932    8    21       102               32               1   \n",
       "1933    1    24       116               25               0   \n",
       "\n",
       "      RAVLT_perc_forgetting  mPACCdigit  EcogPtMem  EcogPtLang  EcogPtVisspat  \\\n",
       "0                  100.0000   -20.06920    3.14286    3.000000       3.000000   \n",
       "1                   83.3333   -10.20060    2.10000    1.780554       1.274284   \n",
       "2                  -18.1818    -5.90200    2.75000    2.555560       2.285710   \n",
       "3                   20.0000     3.19941    1.75000    1.333330       1.000000   \n",
       "4                   28.5714    -1.16303    1.87500    1.555560       1.000000   \n",
       "...                     ...         ...        ...         ...            ...   \n",
       "1929               100.0000   -10.37820    2.97500    1.930556       1.585714   \n",
       "1930                42.8571    -9.18102    2.12500    1.125000       1.166670   \n",
       "1931                16.6667    -9.94141    1.60000    1.533332       1.219048   \n",
       "1932               100.0000   -13.05080    1.87500    1.333330       1.000000   \n",
       "1933                25.0000    -9.22386    2.12500    1.666670       1.000000   \n",
       "\n",
       "      EcogPtPlan  EcogPtOrgan  EcogPtDivatt  EcogSPMem  EcogSPLang  \\\n",
       "0       3.200000     2.500000          2.75   4.000000    3.444440   \n",
       "1       1.373334     1.699998          1.65   2.950000    2.530556   \n",
       "2       3.200000     3.833330          3.50   1.510714    1.266666   \n",
       "3       1.000000     1.166670          1.00   1.000000    1.000000   \n",
       "4       1.000000     1.333330          1.75   1.375000    1.111110   \n",
       "...          ...          ...           ...        ...         ...   \n",
       "1929    1.400000     1.416668          1.95   3.025000    1.955554   \n",
       "1930    2.000000     2.500000          2.00   2.625000    1.625000   \n",
       "1931    1.040000     1.133334          1.45   2.100000    1.322222   \n",
       "1932    1.200000     1.333330          1.00   3.125000    2.875000   \n",
       "1933    1.000000     1.166670          2.00   1.750000    1.714290   \n",
       "\n",
       "      EcogSPVisspat  EcogSPPlan  EcogSPOrgan  EcogSPDivatt       FDG  \\\n",
       "0          2.666670    3.000000     3.666670          3.75  1.042620   \n",
       "1          2.014286    1.756666     2.316666          2.75  1.080580   \n",
       "2          1.438096    1.760000     1.680000          1.85  1.414550   \n",
       "3          1.000000    1.000000     1.000000          1.00  1.118820   \n",
       "4          1.000000    1.200000     1.166670          1.75  1.270141   \n",
       "...             ...         ...          ...           ...       ...   \n",
       "1929       1.671428    1.600000     1.866666          2.35  1.096238   \n",
       "1930       1.166670    1.400000     1.833330          2.00  1.068610   \n",
       "1931       1.657144    1.400000     1.366666          2.05  1.130734   \n",
       "1932       1.857140    1.333330     2.250000          3.25  1.116780   \n",
       "1933       1.200000    1.250000     1.000000          1.00  1.018180   \n",
       "\n",
       "      TAU/ABETA  PTAU/ABETA  Hippocampus/ICV  Entorhinal/ICV  Fusiform/ICV  \\\n",
       "0      0.630939    0.061115         0.003517        0.000970      0.009514   \n",
       "1      0.421770    0.041719         0.003360        0.001276      0.008863   \n",
       "2      0.182691    0.016757         0.005090        0.002786      0.012161   \n",
       "3      0.263255    0.024724         0.004908        0.002758      0.012983   \n",
       "4      0.197315    0.018091         0.004700        0.002912      0.013675   \n",
       "...         ...         ...              ...             ...           ...   \n",
       "1929   0.709354    0.067789         0.003776        0.001849      0.010658   \n",
       "1930   0.263624    0.025132         0.003199        0.001527      0.012497   \n",
       "1931   0.175266    0.015387         0.005291        0.002843      0.009460   \n",
       "1932   0.673073    0.062445         0.002890        0.001061      0.008794   \n",
       "1933   0.314876    0.034044         0.003821        0.002456      0.011175   \n",
       "\n",
       "      MidTemp/ICV  Ventricles/ICV  WholeBrain/ICV  \n",
       "0        0.011188        0.039862        0.680575  \n",
       "1        0.010643        0.049299        0.640478  \n",
       "2        0.014206        0.025251        0.630110  \n",
       "3        0.013293        0.044823        0.677111  \n",
       "4        0.016983        0.036393        0.726675  \n",
       "...           ...             ...             ...  \n",
       "1929     0.011583        0.019025        0.659038  \n",
       "1930     0.014003        0.030409        0.648833  \n",
       "1931     0.013091        0.019144        0.634472  \n",
       "1932     0.009382        0.040972        0.642200  \n",
       "1933     0.011473        0.052054        0.720471  \n",
       "\n",
       "[1934 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = train.reindex(columns=['DX', \"AGE\", \"PTGENDER\", \"PTEDUCAT\", \"APOE4\", \"MMSE\", \"CDRSB\", \"ADAS13\", \"LDELTOTAL\", \"FAQ\", \"MOCA\", \"TRABSCOR\", \"RAVLT_immediate\", \"RAVLT_learning\", \"RAVLT_perc_forgetting\", \"mPACCdigit\", \"EcogPtMem\", \"EcogPtLang\", \"EcogPtVisspat\", \"EcogPtPlan\", \"EcogPtOrgan\", \"EcogPtDivatt\", \"EcogSPMem\", \"EcogSPLang\", \"EcogSPVisspat\", \"EcogSPPlan\", \"EcogSPOrgan\", \"EcogSPDivatt\", \"FDG\", \"TAU/ABETA\", \"PTAU/ABETA\", \"Hippocampus/ICV\", \"Entorhinal/ICV\", \"Fusiform/ICV\", \"MidTemp/ICV\", \"Ventricles/ICV\", \"WholeBrain/ICV\"])\n",
    "test = test.reindex(columns=['DX', \"AGE\", \"PTGENDER\", \"PTEDUCAT\", \"APOE4\", \"MMSE\", \"CDRSB\", \"ADAS13\", \"LDELTOTAL\", \"FAQ\", \"MOCA\", \"TRABSCOR\", \"RAVLT_immediate\", \"RAVLT_learning\", \"RAVLT_perc_forgetting\", \"mPACCdigit\", \"EcogPtMem\", \"EcogPtLang\", \"EcogPtVisspat\", \"EcogPtPlan\", \"EcogPtOrgan\", \"EcogPtDivatt\", \"EcogSPMem\", \"EcogSPLang\", \"EcogSPVisspat\", \"EcogSPPlan\", \"EcogSPOrgan\", \"EcogSPDivatt\", \"FDG\", \"TAU/ABETA\", \"PTAU/ABETA\", \"Hippocampus/ICV\", \"Entorhinal/ICV\", \"Fusiform/ICV\", \"MidTemp/ICV\", \"Ventricles/ICV\", \"WholeBrain/ICV\"])\n",
    "\n",
    "train.to_csv('../data/train.csv', index=False)\n",
    "test.to_csv('../data/test.csv', index=False)\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the dataset given by the Data Preprocessing procedure with sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>mPACCdigit</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>FDG</th>\n",
       "      <th>TAU/ABETA</th>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>60.680331</td>\n",
       "      <td>-5.999106</td>\n",
       "      <td>2.409017</td>\n",
       "      <td>1.687067</td>\n",
       "      <td>1.038877</td>\n",
       "      <td>1.508853</td>\n",
       "      <td>1.363279</td>\n",
       "      <td>2.658198</td>\n",
       "      <td>1.727050</td>\n",
       "      <td>1.383967</td>\n",
       "      <td>1.103983</td>\n",
       "      <td>1.545574</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.476231</td>\n",
       "      <td>1.207387</td>\n",
       "      <td>0.391996</td>\n",
       "      <td>0.033913</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.026616</td>\n",
       "      <td>0.696178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>94</td>\n",
       "      <td>67</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.809160</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>1.444442</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.066666</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>1.066666</td>\n",
       "      <td>1.066666</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.355332</td>\n",
       "      <td>0.330964</td>\n",
       "      <td>0.031714</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.012297</td>\n",
       "      <td>0.013312</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.691732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1.5</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>71.428600</td>\n",
       "      <td>-7.933560</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.755556</td>\n",
       "      <td>1.361906</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.433332</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.328572</td>\n",
       "      <td>1.877778</td>\n",
       "      <td>2.104760</td>\n",
       "      <td>1.760000</td>\n",
       "      <td>1.876666</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>1.097973</td>\n",
       "      <td>0.502035</td>\n",
       "      <td>0.055375</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.011503</td>\n",
       "      <td>0.023139</td>\n",
       "      <td>0.610266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>98</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>-3.140760</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>1.777780</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.345940</td>\n",
       "      <td>0.122598</td>\n",
       "      <td>0.010967</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.018772</td>\n",
       "      <td>0.702045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-14.250300</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.666670</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.072556</td>\n",
       "      <td>0.439187</td>\n",
       "      <td>0.042353</td>\n",
       "      <td>0.003236</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.010869</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.620433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>259</td>\n",
       "      <td>37</td>\n",
       "      <td>7</td>\n",
       "      <td>-36.363600</td>\n",
       "      <td>-4.647250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.444440</td>\n",
       "      <td>1.428570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.111110</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.380370</td>\n",
       "      <td>0.303912</td>\n",
       "      <td>0.027509</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>0.013284</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.734359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>7.142860</td>\n",
       "      <td>-0.317317</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>2.133334</td>\n",
       "      <td>1.766668</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>2.066668</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>2.025000</td>\n",
       "      <td>1.627780</td>\n",
       "      <td>1.506668</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.926666</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>1.172210</td>\n",
       "      <td>0.472523</td>\n",
       "      <td>0.048997</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>0.024570</td>\n",
       "      <td>0.678846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0.5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>-1.984110</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.072410</td>\n",
       "      <td>0.149565</td>\n",
       "      <td>0.013703</td>\n",
       "      <td>0.005109</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.013422</td>\n",
       "      <td>0.020608</td>\n",
       "      <td>0.701561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>LMCI</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>-6.132330</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.888890</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.666670</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>1.888890</td>\n",
       "      <td>1.285710</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.280490</td>\n",
       "      <td>0.135061</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.012528</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.714705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>EMCI</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>-2.362520</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.455630</td>\n",
       "      <td>0.371793</td>\n",
       "      <td>0.033969</td>\n",
       "      <td>0.006188</td>\n",
       "      <td>0.002473</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>0.013007</td>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.750392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DX  AGE  PTGENDER  PTEDUCAT  APOE4  MMSE  CDRSB  ADAS13  LDELTOTAL  \\\n",
       "0     EMCI   78         0        20      0    27    0.5      13          9   \n",
       "1       CN   74         0        16      1    30    0.0       1         17   \n",
       "2     LMCI   79         1        14      1    26    1.5      16          2   \n",
       "3     EMCI   70         0        16      0    30    0.5      13          5   \n",
       "4     LMCI   76         1        15      1    26    5.0      28          0   \n",
       "...    ...  ...       ...       ...    ...   ...    ...     ...        ...   \n",
       "1995  EMCI   77         0        17      0    29    1.0      14          6   \n",
       "1996  LMCI   70         1        18      1    30    1.5       9          8   \n",
       "1997  EMCI   71         1        18      0    28    0.5      11         10   \n",
       "1998  LMCI   67         1        18      0    29    1.0      15          6   \n",
       "1999  EMCI   63         0        12      1    29    1.0       8          8   \n",
       "\n",
       "      FAQ  MOCA  TRABSCOR  RAVLT_immediate  RAVLT_learning  \\\n",
       "0       0    23        82               32               6   \n",
       "1       0    29        94               67               6   \n",
       "2       4    21        65               30               4   \n",
       "3       0    25        98               38               4   \n",
       "4       7    20        54               13               3   \n",
       "...   ...   ...       ...              ...             ...   \n",
       "1995    0    20       259               37               7   \n",
       "1996    0    26        59               50               9   \n",
       "1997    5    24        75               41               6   \n",
       "1998    0    25        63               40               3   \n",
       "1999    0    24        48               38               5   \n",
       "\n",
       "      RAVLT_perc_forgetting  mPACCdigit  EcogPtMem  EcogPtLang  EcogPtVisspat  \\\n",
       "0                 60.680331   -5.999106   2.409017    1.687067       1.038877   \n",
       "1                  0.000000    2.809160   1.625000    1.444442       1.200000   \n",
       "2                 71.428600   -7.933560   2.000000    1.755556       1.361906   \n",
       "3                 87.500000   -3.140760   2.375000    1.777780       1.142860   \n",
       "4                100.000000  -14.250300   2.500000    2.666670       2.000000   \n",
       "...                     ...         ...        ...         ...            ...   \n",
       "1995             -36.363600   -4.647250   2.000000    1.444440       1.428570   \n",
       "1996               7.142860   -0.317317   2.450000    2.133334       1.766668   \n",
       "1997              40.000000   -1.984110   2.000000    2.000000       1.166670   \n",
       "1998              75.000000   -6.132330   2.250000    1.888890       1.142860   \n",
       "1999              70.000000   -2.362520   1.375000    1.000000       1.000000   \n",
       "\n",
       "      EcogPtPlan  EcogPtOrgan  EcogPtDivatt  EcogSPMem  EcogSPLang  \\\n",
       "0       1.508853     1.363279      2.658198   1.727050    1.383967   \n",
       "1       1.200000     1.066666      1.600000   1.150000    1.066666   \n",
       "2       1.200000     1.433332      1.250000   2.328572    1.877778   \n",
       "3       1.600000     1.000000      1.250000   1.375000    1.333330   \n",
       "4       2.000000     2.000000      2.000000   2.250000    2.000000   \n",
       "...          ...          ...           ...        ...         ...   \n",
       "1995    1.000000     1.333330      1.000000   1.250000    1.111110   \n",
       "1996    1.480000     2.066668      2.150000   2.025000    1.627780   \n",
       "1997    1.000000     1.000000      1.000000   2.625000    1.666670   \n",
       "1998    1.400000     1.666670      3.000000   2.375000    1.888890   \n",
       "1999    1.200000     1.000000      1.500000   1.250000    1.000000   \n",
       "\n",
       "      EcogSPVisspat  EcogSPPlan  EcogSPOrgan  EcogSPDivatt       FDG  \\\n",
       "0          1.103983    1.545574     1.000000      1.476231  1.207387   \n",
       "1          1.066666    1.080000     1.200000      1.250000  1.355332   \n",
       "2          2.104760    1.760000     1.876666      2.450000  1.097973   \n",
       "3          1.142860    1.200000     1.000000      1.250000  1.345940   \n",
       "4          1.666670    1.400000     2.000000      2.000000  1.072556   \n",
       "...             ...         ...          ...           ...       ...   \n",
       "1995       1.142860    1.000000     1.000000      1.500000  1.380370   \n",
       "1996       1.506668    1.750000     1.926666      2.200000  1.172210   \n",
       "1997       1.666670    2.400000     1.166670      2.250000  1.072410   \n",
       "1998       1.285710    1.400000     1.333330      2.250000  1.280490   \n",
       "1999       1.000000    1.000000     1.000000      1.250000  1.455630   \n",
       "\n",
       "      TAU/ABETA  PTAU/ABETA  Hippocampus/ICV  Entorhinal/ICV  Fusiform/ICV  \\\n",
       "0      0.391996    0.033913         0.004632        0.002612      0.012346   \n",
       "1      0.330964    0.031714         0.005376        0.002775      0.012297   \n",
       "2      0.502035    0.055375         0.003822        0.002227      0.008753   \n",
       "3      0.122598    0.010967         0.005470        0.002888      0.012538   \n",
       "4      0.439187    0.042353         0.003236        0.001600      0.010214   \n",
       "...         ...         ...              ...             ...           ...   \n",
       "1995   0.303912    0.027509         0.004605        0.002441      0.014253   \n",
       "1996   0.472523    0.048997         0.004844        0.002562      0.012640   \n",
       "1997   0.149565    0.013703         0.005109        0.002793      0.012876   \n",
       "1998   0.135061    0.011992         0.005751        0.002887      0.012528   \n",
       "1999   0.371793    0.033969         0.006188        0.002473      0.014828   \n",
       "\n",
       "      MidTemp/ICV  Ventricles/ICV  WholeBrain/ICV  \n",
       "0        0.013350        0.026616        0.696178  \n",
       "1        0.013312        0.008794        0.691732  \n",
       "2        0.011503        0.023139        0.610266  \n",
       "3        0.014535        0.018772        0.702045  \n",
       "4        0.010869        0.032258        0.620433  \n",
       "...           ...             ...             ...  \n",
       "1995     0.013284        0.019592        0.734359  \n",
       "1996     0.014303        0.024570        0.678846  \n",
       "1997     0.013422        0.020608        0.701561  \n",
       "1998     0.014558        0.015298        0.714705  \n",
       "1999     0.013007        0.005994        0.750392  \n",
       "\n",
       "[2000 rows x 37 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampled = sampled.reindex(columns=['DX', \"AGE\", \"PTGENDER\", \"PTEDUCAT\", \"APOE4\", \"MMSE\", \"CDRSB\", \"ADAS13\", \"LDELTOTAL\", \"FAQ\", \"MOCA\", \"TRABSCOR\", \"RAVLT_immediate\", \"RAVLT_learning\", \"RAVLT_perc_forgetting\", \"mPACCdigit\", \"EcogPtMem\", \"EcogPtLang\", \"EcogPtVisspat\", \"EcogPtPlan\", \"EcogPtOrgan\", \"EcogPtDivatt\", \"EcogSPMem\", \"EcogSPLang\", \"EcogSPVisspat\", \"EcogSPPlan\", \"EcogSPOrgan\", \"EcogSPDivatt\", \"FDG\", \"TAU/ABETA\", \"PTAU/ABETA\", \"Hippocampus/ICV\", \"Entorhinal/ICV\", \"Fusiform/ICV\", \"MidTemp/ICV\", \"Ventricles/ICV\", \"WholeBrain/ICV\"])\n",
    "\n",
    "sampled.to_csv('../data/sampled.csv', index=False)\n",
    "\n",
    "display(sampled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
