{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be building classification models on preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>MARRIED</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>ADAS13</th>\n",
       "      <th>LDELTOTAL</th>\n",
       "      <th>FAQ</th>\n",
       "      <th>MOCA</th>\n",
       "      <th>TRABSCOR</th>\n",
       "      <th>RAVLT_immediate</th>\n",
       "      <th>RAVLT_learning</th>\n",
       "      <th>RAVLT_perc_forgetting</th>\n",
       "      <th>mPACCdigit</th>\n",
       "      <th>EcogPtMem</th>\n",
       "      <th>EcogPtLang</th>\n",
       "      <th>EcogPtVisspat</th>\n",
       "      <th>EcogPtPlan</th>\n",
       "      <th>EcogPtOrgan</th>\n",
       "      <th>EcogPtDivatt</th>\n",
       "      <th>EcogSPMem</th>\n",
       "      <th>EcogSPLang</th>\n",
       "      <th>EcogSPVisspat</th>\n",
       "      <th>EcogSPPlan</th>\n",
       "      <th>EcogSPOrgan</th>\n",
       "      <th>EcogSPDivatt</th>\n",
       "      <th>FDG</th>\n",
       "      <th>TAU/ABETA</th>\n",
       "      <th>PTAU/ABETA</th>\n",
       "      <th>Hippocampus/ICV</th>\n",
       "      <th>Entorhinal/ICV</th>\n",
       "      <th>Fusiform/ICV</th>\n",
       "      <th>MidTemp/ICV</th>\n",
       "      <th>Ventricles/ICV</th>\n",
       "      <th>WholeBrain/ICV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>13.3333</td>\n",
       "      <td>4.26323</td>\n",
       "      <td>1.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.142860</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.542050</td>\n",
       "      <td>0.092561</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.005651</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.758435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>108</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-13.33150</td>\n",
       "      <td>2.175</td>\n",
       "      <td>1.955556</td>\n",
       "      <td>1.466666</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.850</td>\n",
       "      <td>2.196826</td>\n",
       "      <td>1.519048</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.873334</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>1.130270</td>\n",
       "      <td>0.535992</td>\n",
       "      <td>0.053555</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.008471</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.035284</td>\n",
       "      <td>0.560299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>195</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>-50.0000</td>\n",
       "      <td>-3.04793</td>\n",
       "      <td>1.750</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.166670</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.500</td>\n",
       "      <td>1.222220</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.366230</td>\n",
       "      <td>0.216769</td>\n",
       "      <td>0.019595</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.013432</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.024193</td>\n",
       "      <td>0.688609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>87.5000</td>\n",
       "      <td>-1.48431</td>\n",
       "      <td>2.250</td>\n",
       "      <td>1.555560</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.160760</td>\n",
       "      <td>0.132848</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.004594</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.032013</td>\n",
       "      <td>0.640380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>77</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-1.33310</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>1.714290</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.500</td>\n",
       "      <td>1.444440</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.218897</td>\n",
       "      <td>0.269023</td>\n",
       "      <td>0.025773</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.014347</td>\n",
       "      <td>0.021727</td>\n",
       "      <td>0.674128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>3</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>5.5</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>300</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-17.76600</td>\n",
       "      <td>2.375</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>1.495236</td>\n",
       "      <td>1.56</td>\n",
       "      <td>1.466668</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.725</td>\n",
       "      <td>3.177778</td>\n",
       "      <td>3.090476</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.499998</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>0.955939</td>\n",
       "      <td>0.776183</td>\n",
       "      <td>0.077679</td>\n",
       "      <td>0.003527</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.007898</td>\n",
       "      <td>0.035978</td>\n",
       "      <td>0.552868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>200</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-16.34210</td>\n",
       "      <td>2.800</td>\n",
       "      <td>1.822224</td>\n",
       "      <td>2.172380</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.899998</td>\n",
       "      <td>2.20</td>\n",
       "      <td>3.650</td>\n",
       "      <td>2.857142</td>\n",
       "      <td>3.271428</td>\n",
       "      <td>3.52</td>\n",
       "      <td>3.533334</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>1.130688</td>\n",
       "      <td>0.528437</td>\n",
       "      <td>0.051576</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.009186</td>\n",
       "      <td>0.010679</td>\n",
       "      <td>0.052932</td>\n",
       "      <td>0.629352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-13.87190</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.777780</td>\n",
       "      <td>2.428570</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.500</td>\n",
       "      <td>1.555560</td>\n",
       "      <td>1.428570</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.333330</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.986421</td>\n",
       "      <td>0.609316</td>\n",
       "      <td>0.063925</td>\n",
       "      <td>0.004964</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.701381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>76</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>23.0769</td>\n",
       "      <td>-6.68925</td>\n",
       "      <td>1.375</td>\n",
       "      <td>1.222220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.372170</td>\n",
       "      <td>0.252211</td>\n",
       "      <td>0.023819</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.013041</td>\n",
       "      <td>0.014044</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>0.728075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5.0</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>114</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>-12.71650</td>\n",
       "      <td>3.000</td>\n",
       "      <td>2.027780</td>\n",
       "      <td>1.528570</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.466666</td>\n",
       "      <td>2.533334</td>\n",
       "      <td>2.88</td>\n",
       "      <td>2.966666</td>\n",
       "      <td>2.883334</td>\n",
       "      <td>0.970164</td>\n",
       "      <td>0.553113</td>\n",
       "      <td>0.049934</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.011730</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>0.531598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1934 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DX  AGE  PTGENDER  PTEDUCAT  MARRIED  APOE4  MMSE  CDRSB  ADAS13  \\\n",
       "0      0   56         0        20        1      1    29    0.0       1   \n",
       "1      3   84         1        16        1      0    25    5.0      23   \n",
       "2      1   71         1        18        1      0    29    1.5      15   \n",
       "3      0   74         1        16        1      0    30    0.0      13   \n",
       "4      2   78         1        18        1      0    30    0.5      15   \n",
       "...   ..  ...       ...       ...      ...    ...   ...    ...     ...   \n",
       "1929   3   84         1        15        0      1    21    5.5      38   \n",
       "1930   3   79         0        12        0      0    22    8.0      26   \n",
       "1931   2   56         0        18        1      0    25    1.5      18   \n",
       "1932   2   61         1        20        1      2    27    1.0      11   \n",
       "1933   3   80         1        16        1      1    24    5.0      34   \n",
       "\n",
       "      LDELTOTAL  FAQ  MOCA  TRABSCOR  RAVLT_immediate  RAVLT_learning  \\\n",
       "0            19    0    30        46               57               9   \n",
       "1             0    6    20       108               26               2   \n",
       "2            10    0    26       195               44               3   \n",
       "3            11    0    27        43               27               5   \n",
       "4             8    1    23        77               30               4   \n",
       "...         ...  ...   ...       ...              ...             ...   \n",
       "1929          1   25    13       300               14               0   \n",
       "1930          0   23    17       200               27               3   \n",
       "1931          2    5    22        82               35               1   \n",
       "1932          3    0    28        76               45               9   \n",
       "1933          3   16    19       114               13               3   \n",
       "\n",
       "      RAVLT_perc_forgetting  mPACCdigit  EcogPtMem  EcogPtLang  EcogPtVisspat  \\\n",
       "0                   13.3333     4.26323      1.625    1.000000       1.000000   \n",
       "1                  100.0000   -13.33150      2.175    1.955556       1.466666   \n",
       "2                  -50.0000    -3.04793      1.750    1.333330       1.166670   \n",
       "3                   87.5000    -1.48431      2.250    1.555560       1.000000   \n",
       "4                  100.0000    -1.33310      2.875    2.333330       1.714290   \n",
       "...                     ...         ...        ...         ...            ...   \n",
       "1929               100.0000   -17.76600      2.375    1.850000       1.495236   \n",
       "1930               100.0000   -16.34210      2.800    1.822224       2.172380   \n",
       "1931               100.0000   -13.87190      3.625    1.777780       2.428570   \n",
       "1932                23.0769    -6.68925      1.375    1.222220       1.000000   \n",
       "1933               100.0000   -12.71650      3.000    2.027780       1.528570   \n",
       "\n",
       "      EcogPtPlan  EcogPtOrgan  EcogPtDivatt  EcogSPMem  EcogSPLang  \\\n",
       "0           1.00     1.000000          1.00      1.250    1.000000   \n",
       "1           1.60     1.400000          1.60      2.850    2.196826   \n",
       "2           1.00     1.166670          1.50      1.500    1.222220   \n",
       "3           1.20     1.800000          1.00      1.250    1.000000   \n",
       "4           2.20     2.250000          2.25      2.500    1.444440   \n",
       "...          ...          ...           ...        ...         ...   \n",
       "1929        1.56     1.466668          1.55      3.725    3.177778   \n",
       "1930        1.66     1.899998          2.20      3.650    2.857142   \n",
       "1931        1.80     2.500000          3.25      3.500    1.555560   \n",
       "1932        1.00     1.000000          1.00      1.000    1.000000   \n",
       "1933        1.88     2.200000          2.05      3.300    2.466666   \n",
       "\n",
       "      EcogSPVisspat  EcogSPPlan  EcogSPOrgan  EcogSPDivatt       FDG  \\\n",
       "0          1.142860        1.00     1.000000      1.000000  1.542050   \n",
       "1          1.519048        1.64     1.873334      2.050000  1.130270   \n",
       "2          1.333330        1.60     2.000000      1.500000  1.366230   \n",
       "3          1.000000        1.00     1.000000      1.000000  1.160760   \n",
       "4          1.000000        2.40     1.333330      1.000000  1.218897   \n",
       "...             ...         ...          ...           ...       ...   \n",
       "1929       3.090476        3.40     3.499998      3.550000  0.955939   \n",
       "1930       3.271428        3.52     3.533334      3.800000  1.130688   \n",
       "1931       1.428570        1.40     2.333330      2.750000  0.986421   \n",
       "1932       1.000000        1.00     1.000000      1.000000  1.372170   \n",
       "1933       2.533334        2.88     2.966666      2.883334  0.970164   \n",
       "\n",
       "      TAU/ABETA  PTAU/ABETA  Hippocampus/ICV  Entorhinal/ICV  Fusiform/ICV  \\\n",
       "0      0.092561    0.007846         0.005651        0.002415      0.013687   \n",
       "1      0.535992    0.053555         0.003646        0.001403      0.008471   \n",
       "2      0.216769    0.019595         0.005332        0.002811      0.013432   \n",
       "3      0.132848    0.011846         0.004594        0.002506      0.012798   \n",
       "4      0.269023    0.025773         0.004948        0.002940      0.013130   \n",
       "...         ...         ...              ...             ...           ...   \n",
       "1929   0.776183    0.077679         0.003527        0.001550      0.007621   \n",
       "1930   0.528437    0.051576         0.003261        0.001550      0.009186   \n",
       "1931   0.609316    0.063925         0.004964        0.001718      0.012794   \n",
       "1932   0.252211    0.023819         0.005303        0.003050      0.013041   \n",
       "1933   0.553113    0.049934         0.003128        0.001229      0.009313   \n",
       "\n",
       "      MidTemp/ICV  Ventricles/ICV  WholeBrain/ICV  \n",
       "0        0.016130        0.012542        0.758435  \n",
       "1        0.008796        0.035284        0.560299  \n",
       "2        0.014354        0.024193        0.688609  \n",
       "3        0.012824        0.032013        0.640380  \n",
       "4        0.014347        0.021727        0.674128  \n",
       "...           ...             ...             ...  \n",
       "1929     0.007898        0.035978        0.552868  \n",
       "1930     0.010679        0.052932        0.629352  \n",
       "1931     0.014340        0.013916        0.701381  \n",
       "1932     0.014044        0.011307        0.728075  \n",
       "1933     0.011730        0.060945        0.531598  \n",
       "\n",
       "[1934 rows x 38 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "import shap\n",
    "\n",
    "from CogniPredictAD.visualization import Visualizer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', 116)\n",
    "pd.set_option('display.max_columns', 40)\n",
    "pd.set_option('display.max_info_columns', 40) \n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz = Visualizer(train)\n",
    "viz.count_plot(\n",
    "    x=\"DX\",\n",
    "    figsize=(8, 5),\n",
    "    palette=\"viridis\",\n",
    "    order = [0, 1, 2, 3],\n",
    "    xlabel=\"Diagnosis\",\n",
    "    ylabel=\"Number of Patients\",\n",
    "    title=\"Distribution of Patients by Diagnosis (DX)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already see that: \n",
    "- CN:   689 -> 35.63%\n",
    "- EMCI: 361 -> 18.67%\n",
    "- LMCI: 551 -> 28.48%\n",
    "- AD:   333 -> 17.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_store_models(X_train, y_train, output_dir='../results/all_models'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('logreg', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "        ]),\n",
    "        'Bagging': BaggingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    # Repeated stratified CV\n",
    "    cv_splitter = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "    metrics_list = []\n",
    "    per_class_metrics_list = []\n",
    "    confusion_dict = {}\n",
    "    confusion_norm_dict = {}\n",
    "    roc_dict = {}\n",
    "    saved_model_paths = {}\n",
    "\n",
    "    # Container to store per-fold accuracies for each model\n",
    "    accuracies_per_model = {}\n",
    "\n",
    "    classes = np.unique(y_train)\n",
    "\n",
    "    for clf_name, clf_model in classifiers.items():\n",
    "        print(f\"Training & Evaluating: {clf_name}\")  # progress indicator\n",
    "\n",
    "        true_all = []\n",
    "        pred_all = []\n",
    "        prob_all_list = []        # Collect per-fold probability arrays (aligned)\n",
    "        fold_accuracies = []      # Collect per-fold accuracies\n",
    "\n",
    "        # Cross-validation loop\n",
    "        for train_idx, val_idx in cv_splitter.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            clf_model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = clf_model.predict(X_val_fold)\n",
    "\n",
    "            # Get probability scores\n",
    "            y_prob = clf_model.predict_proba(X_val_fold)\n",
    "            \n",
    "            # Align probabilities columns to classes\n",
    "            prob_cols = getattr(clf_model, \"classes_\", None)\n",
    "\n",
    "            # Create DataFrame and reindex to global `classes` order\n",
    "            prob_df = pd.DataFrame(y_prob, columns=prob_cols)\n",
    "\n",
    "            # Reindex: keep columns in the order of `classes`; missing columns -> filled with 0\n",
    "            prob_df = prob_df.reindex(columns=classes, fill_value=0)\n",
    "\n",
    "            # Now append the aligned numpy array\n",
    "            prob_all_list.append(prob_df.values)\n",
    "\n",
    "            # Aggregate fold results\n",
    "            true_all.extend(y_val_fold)\n",
    "            pred_all.extend(y_pred)\n",
    "\n",
    "            # Compute fold accuracy\n",
    "            fold_acc = accuracy_score(y_val_fold, y_pred)\n",
    "            fold_accuracies.append(fold_acc)\n",
    "\n",
    "        # Store fold accuracies for violin plot\n",
    "        accuracies_per_model[clf_name] = fold_accuracies\n",
    "\n",
    "        # Stack probabilities from all folds\n",
    "        prob_all = np.vstack(prob_all_list) if len(prob_all_list) > 0 else np.empty((0, len(classes)))\n",
    "\n",
    "        true_all = np.array(true_all)\n",
    "        pred_all = np.array(pred_all)\n",
    "\n",
    "        # Global / weighted metrics\n",
    "        clf_metrics = {\n",
    "            'Model': clf_name,\n",
    "            'Accuracy': accuracy_score(true_all, pred_all),\n",
    "            'Precision (weighted)': precision_score(true_all, pred_all, average='weighted', zero_division=0),\n",
    "            'Recall (weighted)': recall_score(true_all, pred_all, average='weighted'),\n",
    "            'F1 Score (weighted)': f1_score(true_all, pred_all, average='weighted'),\n",
    "            'ROC AUC (macro)': roc_auc_score(label_binarize(true_all, classes=classes), prob_all, average='macro', multi_class='ovr')\n",
    "        }\n",
    "        metrics_list.append(clf_metrics)\n",
    "\n",
    "        # Per-class metrics\n",
    "        class_report = classification_report(true_all, pred_all, labels=classes, output_dict=True, zero_division=0)\n",
    "        for cls in classes:\n",
    "            rep = class_report.get(str(cls), {})\n",
    "            per_class_metrics_list.append({\n",
    "                'Model': clf_name,\n",
    "                'Class': cls,\n",
    "                'Precision': rep.get('precision', 0.0),\n",
    "                'Recall': rep.get('recall', 0.0),\n",
    "                'F1 Score': rep.get('f1-score', 0.0),\n",
    "                'Support': rep.get('support', 0)\n",
    "            })\n",
    "\n",
    "        # Confusion matrix (counts and normalized)\n",
    "        cm = confusion_matrix(true_all, pred_all, labels=classes)\n",
    "        confusion_dict[clf_name] = cm\n",
    "\n",
    "        row_sums = cm.sum(axis=1, keepdims=True)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cm_norm = np.divide(cm, row_sums, where=row_sums != 0)\n",
    "            cm_norm = np.nan_to_num(cm_norm)\n",
    "        confusion_norm_dict[clf_name] = cm_norm\n",
    "\n",
    "        # ROC One-vs-Rest per class\n",
    "        y_true_bin = label_binarize(true_all, classes=classes)\n",
    "        fpr_dict = {}\n",
    "        tpr_dict = {}\n",
    "        auc_dict = {}\n",
    "        for i, cls in enumerate(classes):\n",
    "            if y_true_bin[:, i].sum() == 0:\n",
    "                fpr_dict[cls] = np.array([0.0, 1.0])\n",
    "                tpr_dict[cls] = np.array([0.0, 1.0])\n",
    "                auc_dict[cls] = np.nan\n",
    "                continue\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], prob_all[:, i])\n",
    "            auc_val = roc_auc_score(y_true_bin[:, i], prob_all[:, i])\n",
    "            fpr_dict[cls] = fpr\n",
    "            tpr_dict[cls] = tpr\n",
    "            auc_dict[cls] = auc_val\n",
    "        roc_dict[clf_name] = (fpr_dict, tpr_dict, auc_dict)\n",
    "\n",
    "        # Refit model on the full training set before saving\n",
    "        clf_model.fit(X_train, y_train)\n",
    "        joblib.dump(clf_model, os.path.join(output_dir, f\"{clf_name.replace(' ', '_')}.joblib\"), compress=3)\n",
    "        saved_model_paths[clf_name] = os.path.join(output_dir, f\"{clf_name.replace(' ', '_')}.joblib\")\n",
    "\n",
    "    # Results DataFrame\n",
    "    results_df = pd.DataFrame(metrics_list).sort_values('ROC AUC (macro)', ascending=False)\n",
    "    display(results_df)\n",
    "\n",
    "    # Per-class metrics DataFrame\n",
    "    per_class_df = pd.DataFrame(per_class_metrics_list)\n",
    "    display(per_class_df)\n",
    "\n",
    "    # Plot ROC curves: one figure per class (all models compared on that class)\n",
    "    for cls in classes:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for clf_name, (fpr_dict, tpr_dict, auc_dict) in roc_dict.items():\n",
    "            auc_val = auc_dict[cls]\n",
    "            plt.plot(fpr_dict[cls], tpr_dict[cls],\n",
    "                     label=f'{clf_name} (AUC={auc_val:.2f})' if not np.isnan(auc_val) else f'{clf_name} (AUC=nan)')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - Class {cls} (One-vs-Rest)')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot confusion matrices (absolute counts)\n",
    "    n_classifiers = len(confusion_dict)\n",
    "    n_cols = 4\n",
    "    n_rows = n_classifiers // n_cols + int(n_classifiers % n_cols > 0)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    for idx, (clf_name, cm) in enumerate(confusion_dict.items()):\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "        ax.set_title(f'{clf_name} Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    for idx in range(len(confusion_dict), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot normalized confusion matrices\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5 * n_rows))\n",
    "    axes = axes.flatten()\n",
    "    for idx, (clf_name, cm_norm) in enumerate(confusion_norm_dict.items()):\n",
    "        ax = axes[idx]\n",
    "        sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', cbar=False, ax=ax)\n",
    "        ax.set_title(f'{clf_name} Normalized Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    for idx in range(len(confusion_norm_dict), len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Violin plot of per-fold accuracies for each model\n",
    "    violin_rows = []\n",
    "    for model_name, acc_list in accuracies_per_model.items():\n",
    "        for acc in acc_list:\n",
    "            violin_rows.append({'Model': model_name, 'Accuracy': acc})\n",
    "    if violin_rows:\n",
    "        violin_df = pd.DataFrame(violin_rows)\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.violinplot(x='Model', y='Accuracy', data=violin_df, inner='quartile')\n",
    "        plt.xlabel('Model')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Model Accuracy Comparison (per-fold distributions)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gridsearch(X_train, y_train, cv=5, scoring='f1_weighted'):\n",
    "    \"\"\"\n",
    "    Runs GridSearchCV on multiple classifiers with their respective parameter grids.\n",
    "    The classifiers and parameter grids are predefined inside the function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like\n",
    "        Training feature matrix.\n",
    "    y_train : array-like\n",
    "        Training target vector.\n",
    "    cv : int, default=5\n",
    "        Number of folds for cross-validation.\n",
    "    scoring : str, default='f1_macro'\n",
    "        Scoring metric to optimize (e.g., 'accuracy', 'roc_auc', 'f1_macro').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_models : dict\n",
    "        Dictionary containing best estimator, parameters, and score for each classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    # === Classifiers definitions ===\n",
    "    classifiers = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Extra Trees': ExtraTreesClassifier(random_state=42, class_weight='balanced'),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        'XGBoost': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "        'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "        'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('logreg', LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'))\n",
    "        ]),\n",
    "        'Bagging': BaggingClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "    # === Parameter grids ===\n",
    "    param_grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'max_depth': [None, 10, 6, 4],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'max_features': ['sqrt', 0.5, 1]\n",
    "        },\n",
    "        'Extra Trees': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'max_depth': [None, 10, 6, 4],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'max_features': ['sqrt', 0.5, 1]\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'max_depth': [None, 10, 6, 4],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 5, 10]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'learning_rate': [0.3, 0.1, 0.05, 0.01],\n",
    "            'max_depth': [None, 10, 6, 4],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "            'reg_alpha': [0, 1],\n",
    "            'reg_lambda': [0, 1]\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'learning_rate': [0.3, 0.1, 0.05, 0.01],\n",
    "            'max_depth': [None, 10, 6, 4],\n",
    "            'min_child_samples': [5, 20, 50],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "            'reg_alpha': [0, 1],\n",
    "            'reg_lambda': [0, 1]\n",
    "        },\n",
    "        'AdaBoost': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'learning_rate': [1.0, 0.5, 0.1],\n",
    "        },\n",
    "        'Logistic Regression': {\n",
    "            'logreg__C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'logreg__penalty': ['l2']\n",
    "        },\n",
    "        'Bagging': {\n",
    "            'n_estimators': [10, 20, 30, 40, 50],\n",
    "            'max_samples': [0.8, 1.0],\n",
    "            'max_features': [0.5, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # === Run GridSearch for each classifier ===\n",
    "    best_models = {}\n",
    "\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\nRunning GridSearch for {name} ...\")\n",
    "        param_grid = param_grids[name]\n",
    "\n",
    "        # Define GridSearchCV\n",
    "        grid = GridSearchCV(\n",
    "            estimator=clf,\n",
    "            param_grid=param_grid,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,    # Use all available CPU cores\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Fit on training data\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        # Store results\n",
    "        best_models[name] = {\n",
    "            \"best_estimator\": grid.best_estimator_,\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"best_score\": grid.best_score_\n",
    "        }\n",
    "\n",
    "        # Print best params and score\n",
    "        print(f\"Best params for {name}: {grid.best_params_}\")\n",
    "        print(f\"Best {scoring}: {grid.best_score_:.4f}\")\n",
    "\n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running GridSearch for Random Forest ...\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best params for Random Forest: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 10, 'n_estimators': 20}\n",
      "Best f1_weighted: 0.7489\n",
      "\n",
      "Running GridSearch for Extra Trees ...\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "Best params for Extra Trees: {'max_depth': 10, 'max_features': 0.5, 'min_samples_leaf': 5, 'n_estimators': 40}\n",
      "Best f1_weighted: 0.7340\n",
      "\n",
      "Running GridSearch for Decision Tree ...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best params for Decision Tree: {'max_depth': 4, 'min_samples_leaf': 5, 'min_samples_split': 2}\n",
      "Best f1_weighted: 0.6978\n",
      "\n",
      "Running GridSearch for XGBoost ...\n",
      "Fitting 5 folds for each of 1920 candidates, totalling 9600 fits\n",
      "Best params for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': None, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 0, 'subsample': 0.8}\n",
      "Best f1_weighted: 0.7461\n",
      "\n",
      "Running GridSearch for LightGBM ...\n",
      "Fitting 5 folds for each of 5760 candidates, totalling 28800 fits\n",
      "Best params for LightGBM: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': None, 'min_child_samples': 5, 'n_estimators': 40, 'reg_alpha': 1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "Best f1_weighted: 0.7442\n",
      "\n",
      "Running GridSearch for AdaBoost ...\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "Best params for AdaBoost: {'learning_rate': 0.5, 'n_estimators': 50}\n",
      "Best f1_weighted: 0.6821\n",
      "\n",
      "Running GridSearch for Logistic Regression ...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best params for Logistic Regression: {'logreg__C': 10.0, 'logreg__penalty': 'l2'}\n",
      "Best f1_weighted: 0.7252\n",
      "\n",
      "Running GridSearch for Bagging ...\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best params for Bagging: {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 30}\n",
      "Best f1_weighted: 0.7402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Random Forest': {'best_estimator': RandomForestClassifier(class_weight='balanced', max_depth=10, max_features=0.5,\n",
       "                         min_samples_leaf=10, n_estimators=20, random_state=42),\n",
       "  'best_params': {'max_depth': 10,\n",
       "   'max_features': 0.5,\n",
       "   'min_samples_leaf': 10,\n",
       "   'n_estimators': 20},\n",
       "  'best_score': np.float64(0.748851067611455)},\n",
       " 'Extra Trees': {'best_estimator': ExtraTreesClassifier(class_weight='balanced', max_depth=10, max_features=0.5,\n",
       "                       min_samples_leaf=5, n_estimators=40, random_state=42),\n",
       "  'best_params': {'max_depth': 10,\n",
       "   'max_features': 0.5,\n",
       "   'min_samples_leaf': 5,\n",
       "   'n_estimators': 40},\n",
       "  'best_score': np.float64(0.7340070428230571)},\n",
       " 'Decision Tree': {'best_estimator': DecisionTreeClassifier(class_weight='balanced', max_depth=4, min_samples_leaf=5,\n",
       "                         random_state=42),\n",
       "  'best_params': {'max_depth': 4,\n",
       "   'min_samples_leaf': 5,\n",
       "   'min_samples_split': 2},\n",
       "  'best_score': np.float64(0.6977906522178418)},\n",
       " 'XGBoost': {'best_estimator': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric='mlogloss',\n",
       "                feature_types=None, feature_weights=None, gamma=None,\n",
       "                grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                multi_strategy=None, n_estimators=50, n_jobs=None,\n",
       "                num_parallel_tree=None, ...),\n",
       "  'best_params': {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': None,\n",
       "   'n_estimators': 50,\n",
       "   'reg_alpha': 1,\n",
       "   'reg_lambda': 0,\n",
       "   'subsample': 0.8},\n",
       "  'best_score': np.float64(0.746131863327722)},\n",
       " 'LightGBM': {'best_estimator': LGBMClassifier(colsample_bytree=0.8, max_depth=None, min_child_samples=5,\n",
       "                 n_estimators=40, random_state=42, reg_alpha=1, reg_lambda=1,\n",
       "                 subsample=0.8, verbose=-1),\n",
       "  'best_params': {'colsample_bytree': 0.8,\n",
       "   'learning_rate': 0.1,\n",
       "   'max_depth': None,\n",
       "   'min_child_samples': 5,\n",
       "   'n_estimators': 40,\n",
       "   'reg_alpha': 1,\n",
       "   'reg_lambda': 1,\n",
       "   'subsample': 0.8},\n",
       "  'best_score': np.float64(0.7442013352594041)},\n",
       " 'AdaBoost': {'best_estimator': AdaBoostClassifier(learning_rate=0.5, random_state=42),\n",
       "  'best_params': {'learning_rate': 0.5, 'n_estimators': 50},\n",
       "  'best_score': np.float64(0.682122987453899)},\n",
       " 'Logistic Regression': {'best_estimator': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                  ('logreg',\n",
       "                   LogisticRegression(C=10.0, class_weight='balanced',\n",
       "                                      max_iter=1000, random_state=42))]),\n",
       "  'best_params': {'logreg__C': 10.0, 'logreg__penalty': 'l2'},\n",
       "  'best_score': np.float64(0.7251593208649998)},\n",
       " 'Bagging': {'best_estimator': BaggingClassifier(n_estimators=30, random_state=42),\n",
       "  'best_params': {'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 30},\n",
       "  'best_score': np.float64(0.7401880191613966)}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv(\"../data/gg.csv\")\n",
    "# train.drop([\"PTETHNICITY\", \"Hippocampus\", \"Entorhinal\", \"Fusiform\", \"MidTemp\", \"Ventricles\", \"WholeBrain\", \"ICV\", \"RAVLT_forgetting\", \"ABETA\", \"PTAU\", \"TAU\"], axis=1, inplace=True)\n",
    "train.drop(columns=['CDRSB', 'LDELTOTAL', 'mPACCdigit'], axis=1, inplace=True)\n",
    "test.drop(columns=['CDRSB', 'LDELTOTAL', 'mPACCdigit'], axis=1, inplace=True)\n",
    "\n",
    "# Target column\n",
    "y_train = train['DX']\n",
    "\n",
    "# All other columns as features\n",
    "X_train = train.drop(columns=['DX'])\n",
    "\n",
    "run_gridsearch(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CogniPredictAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
