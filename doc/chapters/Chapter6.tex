\section{Model Selection}
The following classification algorithms were chosen:
\begin{enumerate}
	\item \textbf{Decision Tree:} This model constructs a series of "if $\to$ then" rules (split on individual features) to separate classes using a binary tree. Each leaf of the tree corresponds to a prediction. It was chosen because it is immediately interpretable (\textit{XAI}\footnote{Explainable Artificial Intelligence});
	\item \textbf{Random Forest:} Builds many different decision trees on subsamples of the data and averages their predictions. This reduces variance compared to a single tree and improves robustness to noise, outliers, and collinearity;
	\item \textbf{Extra Trees}\footnote{Extremely Randomized Trees}\textbf{:} Similar to Random Forest but chooses more random splits, increasing diversity among trees and often reducing overfitting on noisy features. It was tested to compare with Random Forest and evaluate whether increased randomness improved generalization across the dataset;
	\item \textbf{XGBoost:} A boosting algorithm that builds trees sequentially, each improving the errors of the previous one. It is highly efficient, regularized, and capable of capturing nonlinear interactions between variables, while also controlling overfitting;
	\item \textbf{LightGBM:} A gradient boosting implementation designed to be very fast and scalable. It uses techniques (leaf-wise splitting, binning) that make it particularly efficient on heterogeneous datasets;
	\item \textbf{CatBoost:} A boosting variant that natively handles categorical variables and has robust default hyperparameters to reduce overfitting. It is suitable for working with clinical data;
	\item \textbf{Multinomial Logistic Regression:} A linear model that estimates the probabilities of membership in each class using a linear combination of features. It requires feature standardization to function properly, which was ensured with StandardScaler in the pipeline. It was included as a simple and interpretable statistical baseline, useful for comparing whether the gain from complex models is consistent with a linear solution;
	\item \textbf{Bagging}\footnote{Bootstrap Aggregating}\textbf{:} An ensemble approach that trains several models on different bootstrap samples drawn from the dataset, and then combines their outputs by averaging. The main effect is a reduction in model variance, leading to more stable predictions. It was selected because, in clinical datasets where variability and noise are substantial, bagging provides a straightforward way to assess how much predictive stability can be gained simply by aggregating multiple weak learners.
\end{enumerate}

\vspace{2mm}