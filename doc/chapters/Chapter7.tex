\section{Model Selection}
The following classification algorithms were chosen:
\begin{enumerate}
	\item \textbf{Decision Tree:} This model constructs a series of "if $\to$ then" rules (split on individual features) to separate classes using a binary tree. Each leaf of the tree corresponds to a prediction. It was chosen because it is immediately interpretable (\textit{XAI}\footnote{Explainable Artificial Intelligence});
	\item \textbf{Random Forest:} Builds many different decision trees on subsamples of the data and averages their predictions. This reduces variance compared to a single tree and improves robustness to noise, outliers, and collinearity;
	\item \textbf{Extra Trees}\footnote{Extremely Randomized Trees}\textbf{:} Similar to Random Forest but chooses more random splits, increasing diversity among trees and often reducing overfitting on noisy features. It was tested to compare with Random Forest and evaluate whether increased randomness improved generalization across the dataset;
	\item \textbf{AdaBoost:} A boosting algorithm (Adaptive Boosting) that builds a strong classifier by combining many weak learners (often decision stumps). It iteratively reweights training samples, increasing the weight of those misclassified by previous learners so that subsequent models focus on harder examples. Each weak learner is also given a weight based on its accuracy, and the final prediction is a weighted vote (or sum) of all learners. It tends to reduce bias and can generalize well, though it may be sensitive to noisy data and outliers;
	\item \textbf{Multinomial Logistic Regression:} A linear model that estimates the probabilities of membership in each class using a linear combination of features. It requires feature standardization to function properly, which was ensured with StandardScaler in the pipeline. It was included as a simple and interpretable statistical baseline, useful for comparing whether the gain from complex models is consistent with a linear solution.
\end{enumerate}
