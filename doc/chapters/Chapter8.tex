\section{Hyperparameter Tuning}
Each \textbf{classification algorithm} is \textbf{embedded} in a \textbf{pipeline} that handles \textbf{preprocessing}. We have created an additional pipeline with a \textbf{rebalancing strategy} combining \textbf{RandomUnderSampler} for \textbf{majority classes} and \textbf{SMOTENC} for \textbf{minority ones}. Categorical indices for SMOTENC are explicitly computed relative to \textit{PTGENDER} and \textit{APOE4}. 

\vspace{2mm}

To optimize the performance of the classifiers, the \textbf{hyperparameter tuning} is carried out through exhaustive \textbf{Grid Search} with \textbf{5-fold cross-validation}. The chosen metric is the \textbf{macro-averaged F1 score}, an appropriate criterion when class imbalance is present and when resampling modifies the empirical class distribution.

\vspace{2mm} 

The \textbf{parameter grids} explore regularisation strength for logistic regression, depth and leaf-size constraints for tree-based models, and learning-rate and estimator settings for boosting. The grid search function returns, for each classifier, the optimal configuration and the corresponding cross-validated scores.  

\vspace{2mm}

\textit{Without sampling}, the preferred configurations include: entropy-based decision trees with pruning ($\text{ccp\_alpha}=0.005$), random forests with moderate depth ($\text{max\_depth}=6$) and a relatively small leaf size, and extra-trees models with unrestricted depth but non-trivial leaf constraints. Logistic regression settles on an $\ell_1$ penalty with smaller $C$ values, confirming the advantage of sparsity for this dataset. 

\vspace{2mm}

When \textit{sampling} is introduced, the resulting configurations remain close to the unsampled ones, showing robustness of the underlying modelling assumptions. Random forests and extra-trees select nearly identical depths and feature constraints, whereas logistic regression increases $C$ slightly to account for the modified class distribution. 

\vspace{2mm}

At the end of the Grid Search, the optimal parameters found for each model were used to instantiate the "final" versions of the classifiers and produce subsequent evaluations. 

